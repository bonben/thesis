%!TEX root = ../my_thesis.tex
%\cleardoublepage
\addcontentsline{toc}{chapter}{Résumés}
\chapter*{Résumé}
%\markboth{Résumé}{Résumé}
% put your text here
\vskip1em

Les turbo codes sont une classe de codes correcteurs d'erreurs approchant la limite théorique de capacité formulée par Claude Shannon. Conjointement à leurs excellentes performances de décodage, la complexité calculatoire modérée des turbo décodeurs a permis leur inclusion dans de nombreux standards de communications numériques.

Une des métriques permettant la caractérisation de codes correcteurs d’erreurs est l’évolution du taux d’erreurs binaires en fonction du rapport signal sur bruit. Dans le cadre des turbo codes, une courbe de performance de décodage comprend deux zones principales. Dans la première zone, une faible amélioration de la qualité du canal de transmission entraîne de grandes améliorations au niveau des performances de décodage. En revanche dans la seconde, une amélioration de cette qualité ne résulte qu'en une amélioration marginale des performances de décodage. Cette seconde région est nommée zone du plancher d'erreurs. Elle peut empêcher l'utilisation de turbo codes dans des contextes nécessitant de très faibles taux d'erreurs. C’est pourquoi la communauté scientifique a proposé différentes optimisations favorisant la construction de turbo codes atténuant ce plancher d'erreurs. Cependant, ces approches ne peuvent être considérées pour des turbo codes déjà standardisés.

Dans ce contexte, cette thèse adresse le problème de la réduction du plancher d'erreurs en s'interdisant de modifier la chaîne de communications numériques du côté de l’émetteur. Pour ce faire, un état de l'art de méthodes de post-traitement de décodage est dressé pour les turbo codes. Il apparaît que les solutions efficaces sont coûteuses à mettre en œuvre car elles nécessitent une multiplication des ressources calculatoires ou impactent fortement la latence globale de décodage.

Dans un premier temps, deux algorithmes basés sur une supervision de l'évolution de métriques internes aux décodeurs, sont proposés. L'un deux permet d'augmenter la convergence du turbo décodeur. L'autre ne permet qu'une réduction marginale du plancher d'erreurs. Dans un second temps, il est observé que dans la zone du plancher d'erreurs, les trames décodées par le turbo décodeur sont très proches du mot de code originellement transmis. Ceci est démontré par une proposition de prédiction analytique de la distribution du nombre d'erreurs binaires par trame erronée. Cette dernière est réalisée grâce au spectre de distance du turbo code. Puisque ces erreurs binaires responsables du plancher d'erreur sont peu nombreuses, une métrique permettant de les identifier est mise en œuvre. Ceci mène alors à l'établissement d'un algorithme de décodage permettant de corriger des erreurs résiduelles. Cet algorithme, appelé algorithme Flip-and-Check se base sur un principe de création de mots candidats et de vérifications successives par un code détecteur d'erreurs. Grâce à cet algorithme de décodage, un abaissement du plancher d'erreurs d'un ordre de grandeur est obtenu pour les turbo codes de différents standards (LTE, CCSDS, DVB-RCS et DVB-RCS2), ce, tout en conservant une complexité calculatoire raisonnable.

Finalement, une architecture matérielle de décodage implémentant l’algorithme Flip-and-Check est présentée. Une étude préalable de l'impact des différents paramètres de l'algorithme est menée. Elle aboutit à la définition de valeurs optimales pour certains de ces paramètres. D'autres sont à adapter en fonction des gains visés en terme de performances de décodage. Cette architecture démontre alors la possible intégration de cet algorithme aux turbo décodeurs existants; permettant alors d’abaisser le plancher d'erreurs des différents turbo codes présents dans les différents standards de télécommunication.
\vskip0.5cm
\emph{Mots clefs :} Turbo-codes, Erreurs résiduelles, Post-traitement, Implémentation matérielle




\cleardoublepage
\chapter*{Abstract}
\vskip1em
Since their introduction in the 90's, turbo codes are considered as one of the most powerful error-correcting code. Thanks to their excellent trade-off between computational complexity and decoding performance, they were chosen in many communication standards.

One way to characterize error-correcting codes is the evolution of the bit error rate as a function of signal-to-noise ratio (SNR). The turbo code error rate performance is divided in two different regions: the waterfall region and the error floor region. In the waterfall region, a slight increase in SNR results in a significant drop in error rate. In the error floor region, the error rate performance is only slightly improved as the SNR grows. This error floor can prevent turbo codes from being used in applications with low error rates requirements. Therefore various constructions optimizations that lower the error floor of turbo codes has been proposed in recent years by scientific community. However, these approaches can not be considered for already standardized turbo codes.

This thesis addresses the problem of lowering the error floor of turbo codes without allowing any modification of the digital communication chain at the transmitter side. For this purpose, the state-of-the-art post-processing decoding method for turbo codes is detailed. It appears that efficient solutions are expensive to implement due to the required multiplication of computational resources or can strongly impact the overall decoding latency.

Firstly, two decoding algorithms based on the monitoring of decoder's internal metrics are proposed. The waterfall region is enhanced by the first algorithm. However, the second one marginally lowers the error floor. Then, the study shows that in the error floor region, frames decoded by the turbo decoder are really close to the word originally transmitted. This is demonstrated by a proposition of an analytical prediction of the distribution of the number of bits in errors per erroneous frame. This prediction rests on the distance spectrum of turbo codes. Since the appearance of error floor region is due to only few bits in errors, an identification metric is proposed. This lead to the proposal of an algorithm that can correct residual errors. This algorithm, called Flip-and-Check, rests on the generation of candidate words, followed by verification according to an error-detecting code. Thanks to this decoding algorithm, the error floor of turbo codes encountered in different standards (LTE, CCSDS, DVB-RCS and DVB-RCS2) is lowered by one order of magnitude. This performance improvement is obtained without considering an important computational complexity overhead.

Finally, a hardware decoding architecture implementing the Flip-and-Check algorithm is presented. A preliminary study of the impact of the different parameters of this algorithm is carried out. It leads to the definition of optimal values for some of these parameters. Others has to be adapted according to the gains targeted in terms of decoding performance. The possible integration of this algorithm along with existing turbo decoders is demonstrated thanks to this hardware architecture. This therefore enables the lowering of the error floors of standardized turbo codes.
\vskip0.5cm
\emph{Key words:} Turbo codes, Residual Errors, Post-processing, Hardware architecture
