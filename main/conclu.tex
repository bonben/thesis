%!TEX root = ../my_thesis.tex
\chapter*{Conclusions et perspectives}
\markboth{Conclusions et perspectives}{Conclusions et perspectives}
\addcontentsline{toc}{chapter}{Conclusions et perspectives}
\section*{Conclusions}
Dans un contexte de communications numériques,
le besoin d'obtenir des taux d'erreurs aussi faibles que possible devient primordial pour de nombreux domaines 
applicatifs. Ces travaux de thèse se sont alors concentrés sur l'amélioration des performances de décodage des turbo 
codes et notamment la réduction de leurs planchers d'erreurs. Pour ce faire, différentes techniques associées au turbo 
décodage et des solutions architecturales adaptées ont été proposées.

% De nombreux standards de communications numériques incluent un code CRC concaténé avec le turbo code. Ceci permet 
% d'envisager une plus grande diversité dans le décodage en tirant parti des propriétés de distance de cette concaténation.

Dans le deuxième chapitre, des observations statistiques ont permis de mettre en exergue une corrélation forte entre 
l’occurrence d'oscillations de métriques internes au processus de turbo décodage et la présence d'erreurs à l'issue du
décodage. Cependant, aucune relation d'implication directe entre les occurrences d'oscillations et les erreurs n'a pu
être trouvée. Ceci rend alors délicat l'identification de la position des erreurs à partir de l'observation
des oscillations. Néanmoins, un algorithme, nommé SC EML-MAP, a été proposé afin d'améliorer la pertinence de l'information extrinsèque 
échangée au cours du processus itératif. Cet algorithme est inspiré d'une approche originellement proposée 
dans le cadre du décodage des codes LDPC. Dans le cas de turbo codes binaires, sous couvert d'un nombre maximal d'itérations
important, le seuil de convergence est amélioré par rapport à celui obtenu avec 
l'algorithme EML-MAP fonctionnant avec le même nombre 
d'itérations. Dans le cadre du standard CCSDS, des gains de décodage représentant un ordre de grandeur au niveau du taux 
d'erreur trame ont été observés. Ainsi, pour une valeur de SNR constante les performances de décodage des turbo codes
peuvent être améliorées. Une architecture matérielle a été proposée afin d'estimer le surcoût au niveau de la 
complexité calculatoire de cette approche. Ce surcoût s'est avéré modeste.

Dans le troisième chapitre, un algorithme de correction des erreurs résiduelles, nommé Flip and Check (FNC), a été défini. 
Partant du principe que de nombreux standards de communications numériques incluent un code CRC concaténé avec le turbo 
code, il est possible d'envisager une plus grande diversité dans le décodage en tirant parti des nouvelles propriétés de 
distance de cette concaténation.
L'algorithme FNC comprend trois étapes. Après l'identification des positions les moins fiables dans la trame en cours de traitement par le turbo décodeur,
différents mots candidats sont générés puis vérifiés en utilisant un code détecteur d'erreurs. L’utilisation de l'algorithme FNC permet d'obtenir 
des gains de performance de décodage dans la région du plancher d'erreurs d'au moins un ordre de 
grandeur. Ces gains ont été observés pour des turbo codes binaires ou double binaires à 8 ou 16 états. Les
performances de l'algorithme reposent principalement sur le pouvoir d'identification de la métrique permettant la sélection des positions
les moins fiables dans la séquence d'information reçue. Afin de définir cette métrique, différentes approches
d'identification ont été proposées et comparées. Il apparaît que la métrique mise en exergue permet d'identifier de 
façon quasi-systématique les erreurs résiduelles à l'issue du processus de turbo décodage. Les performances de décodage de
l'algorithme FNC reposent aussi sur le pouvoir de détection du code détecteur d'erreurs associé. En effet, la génération
de multiples mots candidats augmente de fait le risque d'erreur non détectée. Néanmoins la présence
dans les standards de communications numériques de codes CRC dont les tailles atteignent 32 bits permet d'offrir des propriétés 
de distance conséquentes. Ainsi, le risque de non-détection n'impacte pas de manière significative la mise en œuvre de l'algorithme FNC. 

Enfin, le quatrième chapitre détaille une proposition d'architecture matérielle pour l'implémentation de l'algorithme
FNC. L'algorithme FNC pouvant être vu comme une extension du processus de turbo décodage conventionnel, son implantation
peut être réalisée sans impacter le processus itératif de décodage. Cependant, l'interconnexion entre ces deux architectures 
matérielles indépendantes nécessite un transfert de différentes informations du turbo décodeur vers le bloc FNC. Afin
de pouvoir déterminer la manière dont cette interconnexion peut être réalisée, différents ordonnancements de turbo décodage 
ont été présentés dans un premier temps. Puis, une caractérisation de l'impact des différents paramètres de 
l'algorithme FNC a été menée. En effet ces différents paramètres ont un influence sur les performances de décodage de 
l'algorithme et sur la complexité calculatoire résultante. L'objectif a alors été de réduire la complexité calculatoire de 
l'algorithme FNC et de caractériser l'éventuelle dégradation des performances de décodage associée. Cette étude
a été faite dans le cadre du standard LTE. Ces analyses ont permis d'aboutir au cas d'étude d'une architecture 
matérielle associée à l'algorithme FNC pour un ordonnancement de turbo décodage particulier : l'ordonnancement Aller-Retour
avec fenêtre glissante. Les résultats d'implémentation
pour une cible FPGA ont été comparés à un turbo décodeur existant sous forme d'IP Core chez Xilinx. Il apparaît alors que le
surcoût calculatoire induit par l'algorithme FNC est raisonnable et peut être adapté en fonction des gains de
performance de décodage recherchés. Finalement, des propositions quant à la transposition de cette première architecture
matérielle à d'autres types de turbo décodeur ont été introduites. Ces propositions confortent le fait qu'une implémentation matérielle 
de l'algorithme FNC est envisageable, ce quelque soit le turbo décodeur considéré.

Pour conclure sur les travaux de thèse présentés dans ce manuscrit, bien que la présentation originelle des turbo codes remonte 
maintenant à près de 25 ans, la proposition d'algorithmes permettant l'amélioration de leurs performances de décodage
est encore possible. Des algorithmes présentant des gains dans la zone de convergence ou bien dans la zone du plancher
d'erreurs ont été au cœur de ces travaux de thèse. Ces algorithmes ont une complexité calculatoire maîtrisée. Ceci permet
d'envisager dès à présent leur utilisation dans divers contextes applicatifs nécessitant une amélioration des
performances de décodage de turbo codes standardisés. Pour ce faire, elles requièrent la concaténation d'un turbo 
code avec un code détecteur d'erreurs. Il est d'ailleurs intéressant de remarquer qu'en ce qui concerne les codes polaires,
l'algorithme de décodage concentrant une forte émulation auprès de la communauté scientifique est le décodage par liste.
Celui-ci considère l'obtention de $N$ mots candidats. La sélection parmi ceux-ci du mot décidé
est effectuée par une vérification d'un code CRC. Ainsi, la considération 
conjointe des différents éléments constituants une chaîne de communications numériques lors de la réception de l'information
permet d'observer de meilleures performances de décodage. Afin d'illustrer plus encore ce propos, il est possible de citer les chaînes
de communications numériques associant une modulation codée et un code correcteur d'erreurs. De la sorte, le décodage peut être réalisé par un système 
doublement itératif. Dans ce contexte, un échange d'informations extrinsèques est effectué entre une turbo démodulation
et le décodeur de canal, permettant de continuer de s'approcher de la limite théorique de Shannon.

\section*{Perspectives}
Plusieurs perspectives d'études et de recherches restent à explorer à la suite de ce travail de thèse. Ci-après, une liste non
exhaustive de ces perspectives est donnée.

Tout d'abord la première perspective d'étude concerne une limite actuelle des travaux de thèse. Les résultats de simulation
démontrant la pertinence des algorithmes de décodage présentés ont été réalisés pour des canaux de communication
basés exclusivement sur un bruit blanc gaussien. Certes, ce modèle de canal est le plus utilisé par la communauté scientifique
pour comparer les performances de codes correcteurs d'erreurs. Cependant, lors de communications numériques réelles, des 
interférences entre symboles, voire des effacements peuvent apparaître à la réception. Ainsi, considérer des modèles de canaux de transmission 
prenant en compte ces contraintes s'avère nécessaire.

La contribution majeure de ces travaux de thèse repose sur la proposition d'un algorithme de corrections des erreurs 
résiduelles des turbo codes. Cependant, l'apparition d'un plancher d'erreurs semble être rencontré dès lors qu'un 
décodage itératif est considéré. Ainsi, il peut être intéressant d'étendre ces résultats à
d'autres codes correcteurs d'erreurs. Une étude préliminaire semble montrer que dans le 
contexte de certains codes LDPC, une identification des erreurs résiduelles par une métrique semblable à celle 
utilisée au cœur de l'algorithme FNC est possible. Cependant, afin de proposer des gains conséquents en terme de performance de 
décodage, il est nécessaire que les capacités d'identification d'une telle métrique soient particulièrement
efficaces. En ce qui concerne le décodage des codes polaires, grâce à un décodage par liste, avec un nombre de mots
candidats conséquent (32), il semblerait qu'aucun plancher d'erreurs ne soit rencontré pour des taux d'erreurs trame de 
$10^{-10}$. Néanmoins, il est possible que cette constatation diffère pour les codes polaires pressentis pour la 5G. Il est 
en effet difficile d'adresser à la fois un seuil de convergence faible et des performances asymptotiques. De plus, 
la quantification de l'information peut amener l'apparition d'un plancher d'erreurs. Peut-être alors que dans ce 
contexte, une transposition de l'algorithme FNC serait envisageable.

% Au cours du deuxième chapitre, il a été vu que l'algorithme SC EML-MAP ne proposait pas de gain significatif en terme de
% taux d'erreur trame pour un turbo code 16 états lorsqu'un nombre maximum de 8 itérations est considéré (\textit{cf.} 
% Figure \label{fig:sc8it}). En revanche il apparait que le taux d'erreur binaire est dans ce même contexte légèrement 

Enfin, de nombreuses perspectives d'études concernent les aspects architecturaux associés à l'algorithme
FNC. Ainsi, il serait nécessaire d'évaluer au mieux la complexité calculatoire de telles architectures adaptées pour un turbo
décodeur à fort degré de parallélisme. Pour ce faire, des implantations pour une cible technologique de type ASIC sont 
indispensables. Mais avant cela, des travaux quant à la réduction de la complexité calculatoire peuvent encore être
menés. Le principal défi quant à la réduction de la complexité calculatoire de l'algorithme FNC repose sur l'unité de
calculs de CRC. Une première solution a été envisagée à la fin du chapitre 4. En effet, il a été observé que l'efficacité
d'une unité de calcul de CRC augmente avec son degré de parallélisme. Or, cette augmentation de parallélisme est 
concomitante avec une réduction du temps nécessaire pour la vérification d'un CRC. Il apparaît alors opportun de maximiser
le degré de parallélisme de chaque unité de vérification de CRC afin d'en réduire le nombre. Ainsi, une 
seule unité de vérification pourrait alors tester la validité de plusieurs mots candidats durant un même intervalle de temps.

% Tire parti CRC;
% Comme nouvelles approches de décodage de codes polaires. Excellentes performances de décodage sont obtenues en considérant
% un décodage par liste. Ce décodage nécessite un code détecteur d'erreurs afin d'obtenir ces performances.
% Il apparait alors qu'un décodage considérant plusieurs candidats et dont la sélection et effectuée par un code détecteur
% d'erreurs propose d'excellentes performances. 