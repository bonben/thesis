%!TEX root = ../my_thesis.tex

\chapter{Les codes polaires}

Résumé

\vspace*{\fill}
\minitocTITI
\vspace*{\fill}

% \subsection*{Introduction}

\section{Le principe et la construction des codes polaires}

\subsection{Le contexte}
\label{subsec:contexte}


\begin{figure}[t]
\centering
\includegraphics[width=0.8\textwidth]{main/ch1_fig/chaine_com}
\caption{Une chaîne de communication}
\label{fig:chaine_com}
\end{figure}
Une chaîne de communications numériques est représentée en Figure~\ref{fig:chaine_com}.
Elle représente les étapes usuelles de la transmission de données numériques depuis une \textbf{source} vers un \textbf{destinataire} à travers un \textbf{canal}.
Le canal de transmission est le support qui permet le transfert de l'information. Lors de communications sans fil, il s'agit du vide, par lequel passent les ondes émises par les antennes de nos équipements. Or les grandeurs physiques associées à ce canal sont continues tandis que les données à transmettre sont constituées de bits, donc discrètes. Le \textbf{modulateur} transforme ce flux binaire en signaux physiques transmissibles par le canal. Si l'on considère toujours les communications sans fils, le modulateur transforme les séquences de bits en formes d'ondes.

Au sein du canal de communication, les signaux subissent de nombreuses perturbations comme le bruit thermique des composants circuits électroniques de la chaîne de transmission ou encore les interférences causées par d'autres utilisateurs du canal. Le \textbf{démodulateur}, qui a le rôle inverse du modulateur, c'est-à-dire de convertir les signaux physiques en données binaires, peut, à cause de ces perturbations, commettre des erreurs. Ceci est un problème, puisque le but de la chaîne de transmission est de transmettre au destinataire l'exacte réplique de la \textbf{séquence d'information}, $\mathbold{b}$ émise par la source. Lorsque les deux séquences, l'une émise et et l'autre reçue, diffèrent, il y a échec de la transmission. La qualité de la chaîne de transmission est souvent mesurée par son taux d'erreur binaire (BER), qui correspond au nombre de bits différents entre la séquence émise et la séquence reçue.

L'ajout d'un encodeur et d'un décodeur canal dans la chaîne de transmission est un moyen efficace de réduire ce taux d'erreur. L'encodeur transforme une séquence d'information $\mathbold{b}$ de $K$ bits en un \textbf{mot de code} $\mathbold{x}$ de $N$ bits. La taille du mot de code, en nombre de bits, est supérieure à la taille de la séquence d'informations afin d'ajouter de la redondance au message transmis: le rendement du code $R=K/N$ est inférieur à $1$. Cette redondance est utilisée par le décodeur canal, ou parfois le couple démodulateur - décodeur canal, afin de réduire le taux d'erreur binaire de la chaîne de transmission. Dans la présente thèse, le décodeur canal est découplé du démodulateur. L'ensemble modulateur - canal - démodulateur peut être considéré comme une entité indépendante dont l'entrée est le mot de code $\mathbold{x}$ et la sortie une séquence d'estimation $\mathbold{L}$. Cet ensemble est appelé canal composite.

\subsection{Le canal composite}
\label{subsec:canal}

Un seul et unique modèle de canal composite sera considéré tout au long de ce manuscrit. Sa modulation est une modulation à changement de phase binaire (BPSK) qui associe aux valeurs binaires d'entrée $x\in\{0,1\}$ les valeurs réelles $\hat{x}\in\{-1,1\}$, respectivement.

Le canal à bruit blanc additif gaussien à double entrée (BI-AWGNC) tel que défini dans \cite[Section~1.5.1.3]{ryan2009channel} est considéré. Ce modèle est le plus utilisé pour caractériser les performances des codes correcteurs d'erreur car il se rapproche du bruit thermique évoqué précédemment, qui est souvent prépondérant. Il consiste en l'addition aux données de sortie du modulateur $\mathbold{\hat{y}}$ d'une variable réelle à distribution gaussienne centrée en $0$ et de densité spectrale $N_0$. Afin d'évaluer les performances de correction des codes correcteurs d'erreurs, les taux d'erreur seront souvent rapportés à cette densité spectrale, ou plus précisément au \textbf{rapport signal à bruit} (SNR), noté $E_b/N_0$, où $E_b=\frac{\mathbb{E}(\hat{x}^2)}{R}$ est l'énergie moyenne par bit d'information.

Les estimations en sortie du démodulateur sont données sous la forme de rapport de vraisemblance logarithmique (LLR). Leur signe détermine pour chaque donnée $\hat{y}_i \in \mathbold{\hat{y}}$ la valeur binaire d'entrée $\hat{x}_i \in \mathbold{\hat{x}}$ la plus probable. Leur valeur absolue le degré de fiabilité de l'information. Des détails sur le modèle de canal et la définition des LLRs sont données en Annexe \ref{append:decoding_nodes}.

\subsection{L'encodage de codes polaires}
Les codes polaires appartiennent à la famille des codes en blocs linéaire. Soient les ensembles $\mathbb{B}^n = \{0,1\}^n$, et  une matrice de taille $G \in \mathbb{B}^K \times \mathbb{B}^N$. L'encodage d'un code en blocs linéaire est une application linéaire injective de $\mathbb{B}^K$ vers $\mathbb{B}^N$ qui à un élément $\mathbold{b} \in \mathbb{B}^K$ associe un élément $\mathbold{x} \in \mathbb{B}^N$ tel que $x=bG$. Définir un encodeur de code polaire revient donc à définir sa matrice génératrice $G$. 

La matrice génératrice d'un code polaire est elle-même produit de deux matrices $E \in (\mathbb{B}^K \times \mathbb{B}^N)$ et $F^{\otimes n}\in (\mathbb{B}^N)^2$, $G=EF^{\otimes n}$. La multiplication par la matrice $E$ correspond à l'introduction de "bits gelés", dont la valeur est $0$, à la séquence d'information $\mb{b}$. Le résultat de l'opération $\mathbold{u} = E\mathbold{b}$ est constitué de $K$ bits $b_i \in \mathbold{b}$ et de $N-K$ bits gelés. Les positions respectives des bits d'informations et des bits gelés sont liées au phénomène de polarisation décrit dans la section suivante. Ces positions sont déterminantes quant à la performance de correction du code.

La matrice $F^{\otimes n}$, où $N=2^n$, est la n-ième puissance de Kronecker du noyau $F=\left[\begin{smallmatrix} 1 & 0 \\ 1 & 1\end{smallmatrix}\right]$. On peut définir $F^{\otimes n}$ récursivement : $F^{\otimes 1} = F$ et $\forall {n > 1}\text{ , }{F^{\otimes n + 1}=\left[\begin{smallmatrix} F^{\otimes n} & 0_n \\ F^{\otimes n} & F^{\otimes n}\end{smallmatrix}\right]}$ avec $0_n \in \mathbb{B}^N$ une matrice nulle.   En Figure \ref{fig:encodage} sont données plusieurs représentations de cette matrice d'encodage. La représentation sous forme de \textit{factor graph} est une représentation alternative où les symboles $\oplus$ correspondent à des opération \textit{OU-exclusif}. La représentation sous forme d'arbre binaire est quant à elle particulièrement pertinente pour la description des algorithmes de décodage.

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{main/ch1_fig/Graph_N_rec}
\caption{Représentation en \textit{factor graph} de l'encodage de codes polaires}
\label{fig:encodage}
\end{figure}

Le processus d'encodage peut également être modifié pour rendre le code systématique. Un code correcteur d'erreur est dit systématique si les bits de la séquence d'information sont présents dans le mot de code. Pour ce faire, il faut modifier la matrice d'encodage : $G_{sys}=EF^{\otimes n}EF^{\otimes n}$. Il est alors prouvé dans \cite{arikan_systematic_2011} que si $\mb{x}=\mb{b}G_{sys}$, alors $\mb{b}=E^{-1}\mb{x}$. L'encodage systématique est représenté en Figure \ref{fig:sys}.

\begin{figure}[t]
\centering
\includegraphics[width=0.7\textwidth]{main/ch1_fig/Graph_N_sys}
\caption{Encodage systématique}
\label{fig:sys}
\end{figure}

\subsection{Le phénomène de polarisation}
\label{subsec:polarisation}
L'invention des codes polaires et la définition des matrices d'encodage par Ar{\i}kan \cite{arikan_channel_2009} sont liées au phénomène de polarisation. L'ensemble constitué de l'encodeur polaire, du canal composite et du décodeur polaire peut être vu comme un ensemble de canaux de transmission qui chacun transmet un bit. Le terme \og polarisation \fg exprime le fait que ces $N$ canaux ont tendance à se diviser en deux groupes. Un groupe de canaux très fiables, avec une probabilité d'erreur faible, et un groupe de canaux peu fiables, à probabilité d'erreur forte. Cette tendance augmente quand $N$ augmente : les canaux se polarisent. Ainsi, les bits d'informations seront attribués aux canaux fiables, et des bits dits gelés, dont la valeur est connue à priori, seront attribués aux canaux peu fiables. Le processus de détermination de la position des bits gelés dans le vecteur $\mathbold{u}$ est appelé construction des codes polaires. Celle-ci est déterminante pour la performance de correction des codes polaires. Plusieurs méthodes efficaces ont été proposées \cite{tal_how_2013,trifonov_efficient_2012} pour le canal AWGN.

\section{Les algorithmes de décodage de codes polaires}

Deux algorithmes de décodage ont été proposés dès l'invention des codes polaires \cite{arikan_channel_2009}. L'algorithme de décodage appelé Annulation Successive (SC) est un algorithme de décodage à sortie dure : les données de sortie sont des bits. L'algorithme de décodage par propagation de croyance (BP) est au contraire un algorithme de décodage itératif à sortie souple, dont les données de sortie sont des estimations représentée typiquement par des LLRs. De nombreuses variantes de ces algorithme ont ensuite été proposées pour en améliorer les performances de corrections. Les algorithme Annulation Successive par Liste (SCL), Annulation Successive "Flip" (SCF), Annulation Successive par Pile (SCS) sont des évolutions de l'algorithme SC. Au contraire, l'algorithme appelé Annulation Souple (SCAN) peut être vu comme une combinaison des algorithmes SC et BP. Chacun de ces algorithmes est décrit dans cette section.

\subsection{L'algorithme de décodage par Annulation Successive}

% Ajouter channel LLRs à la racine
\subsubsection{Les fonctions élémentaires}

\begin{figure}[t]
  \renewcommand*\thesubfigure{\arabic{subfigure}} 
  \centering
  \subfloat[][Channel LLRs]{\includegraphics[width=.26\textwidth]{main/ch1_fig/sc_graph_1}}\quad\quad
  \subfloat[][$f$ function]{\includegraphics[width=.18\textwidth]{main/ch1_fig/sc_graph_2}}\quad\quad
  \subfloat[][$\texttt{R0}$ function]{\includegraphics[width=.2\textwidth]{main/ch1_fig/sc_graph_3}}\\
  \subfloat[][$g$ function]{\includegraphics[width=.18\textwidth]{main/ch1_fig/sc_graph_4}}\quad\quad
  \subfloat[][$\texttt{R1}$ function]{\includegraphics[width=.2\textwidth]{main/ch1_fig/sc_graph_5}}\quad\quad
  \subfloat[][$h$ function]{\includegraphics[width=.18\textwidth]{main/ch1_fig/sc_graph_6}}
  \caption{Fonctions élémentaires et séquencement du décodage SC du noyau $N=2$}
  \label{fig:sc_steps}
\end{figure}

% Expliciter notation (N,K)
% Mettre cohérence dans numérotation des couches (0 = racine)

%
Les étapes du décodage du \noeud élémentaire de taille 2 sont représentées en Figure~\ref{fig:sc_steps}.
Les données d'entrée des algorithmes de décodages présentés ici sont des LLRs, contenus dans le vecteur $\mathbold{L}$ de taille $N$, sorti du canal composite.
Les données de sorties du décodeur sont des bits contenus dans le vecteur $\mathbold{\hat{b}}$ de taille $K$.
Au cours du décodage de codes polaires, ces deux formats de données (LLRs et bits) sont utilisés.
Huit variables sont nécessaires pour le décodage du noyau élémentaire de taille 2 : 4 LLRs notés $L_{i,j}$ et 4 sommes partielles notés $s{i,j}$.
La première étape (1) du décodage est le chargement des LLRs du canal $L$ : les LLRs $L_{0,0}$ et $L_{1,0}$ prennent la valeur des LLRs du canal. Les LLRs et les sommes partielles de chaque \noeud de l'arbre sont ensuite calculées par l'intermédiaire des opérations $f$, \texttt{R0}, $g$, \texttt{R1} et $h$ symbolisées dans la Figure~\ref{fig:sc_steps} par des flèches. Elles correspondent aux équations \ref{eq:sc} :
\begin{eqnarray}
  \begin{array}{l c l}
    f(L_a,L_b) &=& \text{sign}(L_a.L_b).\min(|L_a|,|L_b|)\\
    g(L_a,L_b,\hat{s}_a)&=&(1-2\hat{s}_a)L_a+L_b\\
    h(\hat{s}_a,\hat{s}_b)&=& (\hat{s}_{a} \oplus \hat{s}_{b}, \hat{s}_{b})\\
    \texttt{R0}(L_a) &=& 0 \\
    \texttt{R1}(L_a) &=&  \left\{\begin{array}{l c l} 0 \text{ si } L_a \geq 0 \\ 1 \text{ si } L_a < 0 \end{array}\right.
  \end{array}
  \label{eq:sc}
\end{eqnarray}
La fonction $f$ est appliquée dans l'étape (2) et permet le calcul de $L_{0,1}$.
L'étape (3) est l'application de la fonction \texttt{R0} sur le \noeud supérieur. 
La fonction \texttt{R0} est appliquée dans l'hypothèse où le bit encodé $u_0$ en Figure \ref{fig:encodage} est un bit gelé.
La fonction $g$ permet ensuite, en étape (4), le calcul de $L_{1,1}$.
L'étape (5) est le calcul de $s_{1,1}$ par l'opération \texttt{R1} correspondant à un bit d'information. 
Cette opération est un seuillage du LLR pour obtenir la somme partielle.
Pour rappel, dans la représentation en LLR, le signe correspond à la décision dure : bit égal à 0 pour un LLR positif, bit égal à 1 pour un LLR négatif. 
La valeur absolue informe sur la fiabilité de cette décision.
Lorsque le seuillage est réalisé, l'information de fiabilité est perdue.
Après le décodage de la somme partielle $s_{1,1}$, la fonction $h$ est appliquée afin de propager les sommes partielles, en étape (6).


\subsubsection{L'arbre de décodage}
\begin{figure}[t]
\centering
\includegraphics[width=0.55\textwidth]{main/ch1_fig/sc}
\caption{Arbre de décodage SC}
\label{fig:sc}
\end{figure}
\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{main/ch1_fig/seq_sc}
\caption{Séquencement du décodage SC d'un code polaire de taille $N=4$.}
\label{fig:seq_sc}
\end{figure}
% mettre en évidence qu'on décode bit par bit, sera utile pour sc list
% introduire termes rendement 1 / rendement 0
% sommes partielles termes non introduit
% vérifier que n est introduit
Lorsque $N$ augmente, il est plus commode d'utiliser la représentation en arbre, comme montré en Figure~\ref{fig:sc}, où $N=8$.
Les données sont organisées en arbre binaire sur $log_2(N) + 1$ \textbf{couches}. Dans notre cas donc, l'arbre possède quatre couches.
A chaque couche sont attribués un certain nombre de \textbf{\noeuds}.
La \textbf{racine} de l'arbre numérotée $0$ est constituée d'un seul \noeud.
La racine contient $N$ LLRs et $N$ sommes partielles.
En descendant dans l'arbre, à chaque couche, le nombre de \noeuds double, tandis que le nombre de LLRs et de sommes partielles de chaque \noeud et divisé par deux.
Les \textbf{feuilles} sont les \noeuds les plus bas de l'arbre.
Le traitement d'une feuille correspond à l'application des opérations \texttt{R0} et \texttt{R1}.
Chaque couche $d$ contient $2^d$ \noeuds constitués de $2^{n-d}$ LLRs et de $2^{n-d}$ sommes partielles, où $n=\log_2(N)$. 
La hauteur jusqu'à laquelle les sommes partielles sont propagées par la fonction $h$ dépend de l'index de la feuille qui vient d'être décodé.

Dans le cas d'un code non systématique, la séquence décodée $\hat{u}$ est composée des sommes partielles contenues dans les feuilles de l'arbre de décodage..
Lors d'un décodage non systématique, les sommes partielles contenues par les feuilles correspondent à la séquence décodée $\hat{u}$.
La Figure~\ref{fig:seq_sc} est une représentation plus synthétique de l'arbre de décodage.
Les opérations 12 et 13 dans la Figure \ref{fig:seq_sc} sont donc inutiles puisqu'elles servent seulement à calculer les sommes partielles du \noeud racine.
Par contre, dans le cas d'un code systématique, ce sont les sommes partielles du \noeud racine qui correspondent au mot de code décodé.
Le parcours de l'arbre est un parcours en profondeur.
La séquence d'application des différentes fonctions est explicitée dans la Figure~\ref{fig:seq_sc}, où chaque flèche représentant une fonction est numérotée dans l'ordre.



\subsubsection{Le parallélisme de l'algorithme}
Dans la Figure~\ref{fig:sc}, il apparaît que quatre fonctions $f$ sont appliquées sur le \noeud racine pour calculer les LLRs de la branche de gauche. Ces quatre fonctions sont indépendantes : leurs entrées et sorties sont disjointes. Par conséquence, elles peuvent être réalisées simultanément. Ce parallélisme existe pour chaque fonction $f$, $g$ et $h$ d'un \noeud donné. Ainsi, le niveau de parallélisme est différent selon la couche considérée et correspond à la taille du \noeud cible. Le parallélisme est donc plus grand en haut de l'arbre qu'en bas. Dans les implémentations logicielles ou matérielles de ces algorithme, ce parallélisme est utilisé pour réduire le temps de décodage et accélérer son débit. Lorsque ce parallélisme est utilisé, on parle de parallélisme \textit{intra-trame}.


\subsection{L'algorithme de décodage par Annulation Successive à Liste}
\subsubsection{L'algorithme}
\begin{figure}[t]
\centering
\includegraphics[width=1\textwidth]{main/ch1_fig/scl}
\caption{Duplication de l'arbre de décodage dans l'algorithme SCL.}
\label{fig:scl}
\end{figure}
L'algorithme de décodage par Annulation Successive Liste (SCL) est une évolution de l'algorithme SC \cite{tal_how_2013}. L'algorithme SC présente en effet des performances de correction médiocres pour des codes polaires de petite taille. Le SCL améliore ces performances substantiellement. Dans l'algorithme de décodage SC décrit précédemment, des décisions dure sont réalisées à chaque traitement d'une feuille correspondant à un bit d'information (fonction \texttt{R1}). Si une erreur est commise lors de ce seuillage, celle-ci est irréversible, et le décodage du mot de code est un échec. Le principe de l'algorithme de décodage par liste est de retarder la décision dure. Au lieu d'appliquer un seuillage sur la valeur du LLR, les deux possibilités de décodage sont considérées, et l'arbre de décodage, avec l'ensemble de ses LLRs et sommes partielles, est dupliqué. Cette duplication est représentée dans la Figure~\ref{fig:scl}. Il est considéré sur la gauche de la figure un décodage au cours duquel la fonction $f$ est appliquée, dans la première étape représentée. L'étape suivante consiste en la duplication de l'arbre de décodage afin de former deux chemins. Dans le premier, l'hypothèse est que $\hat{u}_2=0$, dans le deuxième, $\hat{u}_2=1$ Le décodage continue ainsi sur les deux arbres parallèlement : l'étape suivante dans l'exemple donné est l'application de la fonction $g$. Ces deux arbres sont qualifiés de \textbf{chemins} de décodage. \`A chaque feuille de rendement 1, le même procédé est réalisé, doublant le nombre d'arbres décodés en parallèle.


% citer article balatsoukas pour LLRs et calculs métriques
% Ajouter les bits décodés sur les arbres pour les faire apparaître lors de la description du liste. 
Il n'est pas possible, en pratique, de dupliquer indéfiniment le nombre d'arbres de décodage, la mémoire nécessaire et le nombre de calcul devient vite insoutenable. Un nombre de chemins maximum $L$ est donc paramètre l'algorithme liste. Des La sélection des chemins qui seront conservés ou éliminés est réalisée grâce à une \textbf{métrique} $m$ associée à chaque chemin. Cette métrique est mise à jour à chaque traitement d'une feuille. Le détail des calculs pour un décodage utilisant des LLRs est proposé dans \cite{balatsoukas-stimming_llr-based_2015}. Lorsque la feuille est de rendement 0 si le LLR de la feuille est positif, la métrique est inchangée car l'estimation du LLR est en accord avec la valeur du bit gelé, 0. Si le LLR est négatif alors au contraire, la valeur absolue du LLR lui est ajoutée. Lorsque la feuille est de rendement 1, deux chemins sont créés, avec chacun une version différente du bit décodé. Le chemin créé dont la valeur du bit est en accord avec la valeur du LLR ne reçoit pas de pénalité. Au contraire, la métrique du deuxième chemin créé est accrue de la valeur du LLR.

Une fois ces calculs de métriques réalisés, on sélectionne les chemins à conserver. Si $L$ chemins étaient actifs au moment du traitement d'une feuille de rendement 1, alors $2L$ candidats sont générés, avec chacun une métrique associée. Seuls les $L$ candidats avec la métrique la plus faible sont conservés. Cette opération de sélection est représentée dans la Figure~\ref{fig:scl}. Lors de la dernière étape, à droite de la figure, les deux candidats du bas sont éliminés. L'hypothèse était donc que ces deux chemins avaient un métrique de valeur absolue supérieure aux métriques des deux candidat du haut. \`A la fin du parcours de l'arbre de décodage, $L$ chemins auront donc été conservés. Ces $L$ chemins correspondent à $L$ mots de codes possibles. Une sélection d'un seul candidat parmi les $L$ doit être effectuée. Dans la version originale de l'algorithme, le mot décodé associé au chemin ayant la métrique de valeur la plus faible à la fin du décodage est sélectionné comme étant la sortie de l'algorithme $\mathbold{\hat{b}}$.

% Insistier lourdement sur la signification de chaque terme : chemin, métrique, candidat

\subsubsection{Concaténation avec un CRC}

Les auteurs de \cite{tal_how_2013} ont proposé de concaténer un test de redondance cyclique (CRC) pour discriminer les différents candidats. L'encodage est représenté en Figure~\ref{fig:crc}. \`A la fin de l'algorithme de décodage, lorsque les $L$ mots de codes sont disponibles, un CRC est appliqué sur chacun d'entre eux. Si l'un d'entre eux vérifie le CRC, il est alors hautement probable qu'il soit le bon candidat.

\begin{figure}[t]
\centering
\includegraphics[width=0.6\textwidth]{main/ch1_fig/crc}
\caption{Concaténation avec un CRC.}
\label{fig:crc}
\end{figure}
\subsubsection{Les algorithmes de décodage adaptatifs}

%Citation
L'algorithme SCL adaptatif permet d'augmenter le débit des implémentations l'algorithme CA-SCL. La première étape est d'appliquer l'algorithme SC sur les LLRs de sortie du canal composite. Le test du CRC est alors réalisé. S'il est satisfait, le mot de code est considéré comme bon et le décodage est terminé. Dans le cas contraire, l'algorithme SCL est appliqué. Dans le cas de l'algorithme Partiellement Adaptatif (PA-SCL), l'algorithme est appliqué une seule fois avec comme taille de liste $L_{max}$. Dans le cas de l'algorithme Complètement Adaptatif (FA-SCL), après la première application de l'algorithme SC, l'algorithme CA-SCL est appliqué plusieurs fois. \`A la première itération, $L$ est égal à $2$. Si après décodage, le CRC est satisfait alors l'algorithme FA-SCL s'arrête, sinon il continue en doublant itérativement $L$ jusqu'à atteindre $L_{max}$.

Cet algorithme permet donc d'augmenter le débit, puisque plus $L$ et petit, plus le temps de décodage de l'algorithme SCL est réduit. Cependant, ces algorithmes ont également une latence maximum plus grande par rapport à l'algorithme SCL avec $L=L_{max}$. Soit cette latence, $l_{SCL}$ et la latence de l'algorithme SC, $l_{SC}$, alors dans le Pire Cas (PC), $l^{PC}_{FA-SCL}=l_{SCL}+l_{SC}$

% Prendre une décision sur l'appellation des algos (de / par / [] Annu Succ par / "" / [] Liste / Pile)
% Pour chaque algo, lister possiblement les améliorations mineures
\subsection{Des variantes proches}

	L'algorithme de décodage par Annulation Successive Flip (SCF) \cite{afisiadis_low-complexity_2014} est une variante de l'algorithme SC. Il vise également à améliorer les performances de décodage. Cet algorithme nécessite la concaténation d'un CRC pour être réalisé. L'algorithme de décodage SC est appliqué une première fois. L'ensemble des LLRs permettant les décisions dure (les LLRs des feuilles) est conservé. Si le mot de code décodé satisfait le CRC, alors le décodage s'arrête. Par contre, si le CRC n'est pas satisfait, alors l'algorithme SC est lancé une nouvelle fois. La différence est que la décision prise, lors du premier décodage, sur le LLR le moins fiable est inversée. Ce deuxième mot décodé est encore testé à l'aide du CRC. S'il ne satisfait toujours pas le CRC, une autre séquence de décodage est lancée en inversant le bit du deuxième LLR le moins fiable. Ce mécanisme itératif continue jusqu'à un nombre d'essai maximum $T$.


	% SC Stack
	L'algorithme de décodage par Annulation Successive Pile (SCS) \cite{niu_stack_2012} est une variante de l'algorithme SCL. Dans l'algorithme SCL, le nombre de candidat $L$ est conservé stable tout au long de l'algorithme. De plus, le séquencement est exactement le même que dans le SC dans les différents arbres décodés parallèlement : les feuilles sont décodées l'une après l'autre, dans l'ordre. Dans l'algorithme SCS, les métriques associées aux différents arbres de décodages sont maintenues dans une pile ordonnée de profondeur $T$. L'algorithme SC est appliqué sur le chemin associé à la meilleure métrique. A l'arrivée sur une feuille, la métrique de l'arbre est mise à jour et, si la feuille est de rendement 1, un nouveau chemin est ajouté à la pile. \`A un instant $t$, seul l'arbre étant le plus haut sur la pile est décodé. Lorsque la dernière feuille de l'un des chemins est décodé, le CRC est testé. Le décodage se termine lorsqu'un mot décodé satisfait le CRC ou que le nombre de candidats testés atteint une valeur déterminée à l'avance $D$. L'algorithme SCS présente l'avantage de nécessiter moins d'opérations élémentaires sur les codes polaires. Cependant, le maintien d'une pile et son séquencement particulier rendent son implémentation difficile.
	% Citer Harsch
	% Parler du Stack Hybride ? 

\subsection{Les algorithmes itératifs à sortie souple}

\begin{figure}[t]
  \renewcommand*\thesubfigure{\arabic{subfigure}} 
  \centering
  \subfloat[][Initialisation]{\includegraphics[scale=1.2]{main/ch1_fig/scan_graph_1}}\hspace{2.5cm}
  \subfloat[][$f_{bp}$ function]{\includegraphics[scale=1.2]{main/ch1_fig/scan_graph_2}}\\
  \hspace{0.5cm}
  \subfloat[][$g_{bp}$ function]{\includegraphics[scale=1.2]{main/ch1_fig/scan_graph_3}}\hspace{3.7cm}
  \subfloat[][Backward functions]{\includegraphics[scale=1.2]{main/ch1_fig/scan_graph_4}}
  \caption{Fonctions élémentaires et séquencement du décodage SCAN du noyau $N=2$}
  \label{fig:SCANSchedule}
\end{figure}
% Parler concaténation

Deux algorithmes itératifs à sortie souple ont été proposée. Le premier est appelé algorithme à Propagation de Croyance (BP). C'est le premier dans l'ordre chronologique puisqu'il fut proposé dans l'article original d'Ar{\i}kan \cite{arikan_channel_2009}. Le second fut proposé dans \cite{fayyaz_low-complexity_2014}. Leur séquencement est la seule propriété qui les différencie.

La première différence entre ces deux algorithmes et ceux vues précédemment est qu'aucune décision dure n'est réalisée durant le décodage. L'information reste souple de bout en bout.
En conséquence, les sommes partielles utilisées dans l'arbre de décodage SC sont remplacées par des LLRs. Deux ensembles de LLRs sont donc contenues dans l'arbre de décodage. Les LLRs qui ont remplacé les sommes partielles sont notés $L^b_{i,j}$.
Ensuite, les fonctions élémentaires sont changées. les fonction $f$, $g$ et $h$ de l'algorithme SC sont remplacées par les fonctions $f_{bp}$ et $g_{bp}$ :
\begin{eqnarray}
  \begin{array}{l c l}
    f_{bp}(L_a,L_b,L_c) & = & f(L_a, L_b  + L_c) \\
    g_{bp}(L_a,L_b,L_c) & = & f(L_a, L_c) + L_b
  \end{array}
  \label{eq:bp}
\end{eqnarray}
Le séquencement de ces opérations est montré en figure \ref{fig:SCANSchedule} pour le noyau de taille $N=2$ dans le cas d'un décodage SCAN. La première étape est l'initialisation de la racine et des feuilles de l'arbre : les LLRs $L$ de la racine prennent la valeur des LLRs du canal. Les LLRs $L^b$ des feuilles prennent la valeur $+\infty$ dans le cas d'un bit gelé, et la valeur $0$ dans le cas d'un bit d'information. Ensuite, les fonctions $f_{bp}$ et $g_{bp}$ sont appliquées dans l'ordre indiqué.

Comme évoqué précédemment, les algorithmes SCAN et BP diffèrent du point de vue de leurs séquencement. Le séquencement de l'algorithme SCAN est le même que celui du SC, en remplaçant l'application de la fonction $h$ du SC par une application conjointe des fonctions ${f_bp}$ et $g_{bp}$ comme montré en Figure~\ref{fig:SCANSchedule} (4).

Enfin, les algorithmes BP et SCAN sont des algorithmes itératifs. En effet, le parcours de l'arbre peut être effectué plusieurs fois. Ainsi, les valeurs des LLRs intermédiaires évoluent d'une itération à l'autre. Ces itérations permettent d'améliorer les performances de décodage.

\subsection{Les performances de décodage des différents algorithmes}
% Détails sur les perfs : 32 bit
Dans la présente section sont présentées plusieurs courbes de performances. 
Elles permettent de comparer entre eux les algorithmes présentés précédemment.
Elles montrent également l'impact des paramètres, communs à tous les algorithmes, ou spécifique à chacun, sur les performances de décodage.
Ces paramètres et leurs effets sont important car ce sont les leviers utilisés dans les standards de communication afin de s'adapter aux contraintes extérieures.
Les axes des différentes courbes sont toujours les mêmes.
En abscisse se trouve le rapport signal à bruit noté $E_b/N_0$ décrit en Section \ref{subsec:canal}.
Sur l'axe des ordonnées se trouve soit le BER dont l'expression est donnée en Section \ref{subsec:contexte}, soit le taux d'erreur trame (FER : Frame Error Rate). Le FER est égal au rapport du nombre de trames erronées sur le nombre total de trames émises, sachant qu'une trame est considérée erronée si elle contient au moins un bit erroné.


\begin{figure}[t]
\input{main/ch1_fig/curves/sc_N/tikz/source.tex}
\caption{Performances de décodage de l'algorithme SC pour différentes valeurs de $N$ ($R=1/2$).}
\label{fig:sc_n}
\end{figure}

\begin{figure}[t]
\input{main/ch1_fig/curves/sc_R/tikz/source.tex}
\caption{Performances de décodage de l'algorithme SC pour différentes valeurs de $R$ ($N=2048$).}
\label{fig:sc_r}
\end{figure}

La théorie de l'information de Shannon \cite{shannon_mathematical_2001} prouve qu'il existe une limite théorique à la capacité d'un canal pour un rendement de code donné.
Cette limite est matérialisée dans la Figure \ref{fig:sc_n} par la ligne verticale dénommée \textit{Limite de Shannon}. Les codes polaires sont les premiers qui atteignent théoriquement cette limite. Cependant cette limite n'est atteinte que pour une taille de mot de code infinie.

La Figure~\ref{fig:sc_n} présente les performances de décodage de l'algorithme SC pour une taille de mot code croissante. S'il apparaît clairement que le taux d'erreur binaire diminue pour un rapport signal à bruit donné, la limite de Shannon est toutefois très éloignée pour les tailles de mot de code présentées. 

Les codes correcteurs d'erreur reposent la notion de redondance, quantifiée par le rendement du code. Plus le rendement est faible, plus la redondance est élevée.
Ceci est illustré en Figure~\ref{fig:sc_r}, toujours en considérant l'algorithme SC. La tendance observée est bien que plus le rendement est faible, plus le taux d'erreur est faible.
Une seule exception apparaît, pour le rendement $R=1/8$, rendement le plus faible pourtant. Ceci est dû au fait que l'énergie par bit d'information $E_b$ prend en compte le fait que lorsqu'on diminue le rendement, on augmente l'énergie dépensée pour transmettre un bit utile. Deux tendances s'affrontent donc : augmentation de la redondance contre diminution du rapport signal à bruit.

\begin{figure}[t]
\input{main/ch1_fig/curves/scl_L/tikz/source.tex}
\caption{Performances de décodage des algorithmes SC, SCL et CA-SCL pour un code polaire (2048,1723). Un CRC de taille $c=16$ est utilisé pour l'algorithme CA-SCL.}
\label{fig:scl_l}
\end{figure}

La Figure~\ref{fig:scl_l} introduit les performances de l'algorithme SCL. Comme précisé auparavant, l'algorithme SCL permet d'améliorer les performances de décodage des codes polaires par rapport à l'algorithme SC.
Plusieurs choses sont à observer. Tout d'abord, l'augmentation de la taille de la liste permet dans tous les cas une meilleure correction d'erreur. Ensuite, l'utilisation de la concaténation du CRC par l'algorithme CA-SCL augmente très fortement les performances de décodage. Par exemple lorsque $L=32$, à un FER de 10\textsuperscript{-4}, il y a une différence de presque 1 dB entre les algorithmes SCL et CA-SCL.

Des tendances plus fines se dégagent. Les courbes de l'algorithme SCL montrent que les gains provoqués par l'augmentation du paramètre $L$ arrivent vite à saturation. En effet, les performances obtenues pour $L=8$ et $L=32$ sont très proches, alors que la complexité de l'algorithme augmente approximativement linéairement. Au contraire, concernant l'algorithme CA-SCL dans les conditions données, l'amélioration de performances due à l'augmentation du paramètre $L$ est continue. 

Dans la Figure~\ref{fig:scl_l} sont montrées les courbes de performances de différentes versions de l'algorithme SCL. Tout d'abord, le facteur $L$ est déterminant. En effet plus $L$ est grand plus le taux d'erreurs est faible. Par contre, l'amélioration relative du SCL par rapport au SC est moindre pour de grandes valeurs de $N$. Il apparaît également que l'algorithme CA-SCL, avec l'ajout du CRC, améliore drastiquement les performances de correction par rapport à l'algorithme SCL. Il est important de noter toutefois que la taille dur CRC jour un rôle important dans les performances du code. Tout d'abord, s'il est sous-dimensionné par rapport à $K$, le nombre de bits d'informations, alors il peut apparaître un seuil. Si au contraire il est surdimensionné, alors la courbe de performance se dégrade (décalage vers la droite).


\begin{figure}[t]
\input{main/ch1_fig/curves/scl_crc/tikz/source.tex}
\caption{Impact de la taille du CRC sur les performances de l'algorithme SCL}
\label{fig:scl_crc}
\end{figure}


\begin{figure}[t]
\input{main/ch1_fig/curves/fast_ascl_crc/tikz/source.tex}
\caption{Impact de la taille du CRC sur les performances des l'algorithme ASCL}
\label{fig:ascl_crc}
\end{figure}


\begin{figure}[t]
\input{main/ch1_fig/curves/scf/tikz/source.tex}
\caption{Performances de l'algorithme SCF en comparaison avec les algorithmes SC et SCL pour un code polaire (1024,512), $c=16$}
\label{fig:ascl_crc}
\end{figure}


\begin{figure}[t]
\input{main/ch1_fig/curves/bp_scan/tikz/source.tex}
\caption{Performances des algorithmes itératifs à sortie souple en comparaison avec les algorithmes SC et SCL pour un code polaire (1024,512)}
\label{fig:bp_scan}
\end{figure}




% Courbes avec différents N pour SC et SCL, montrer que SCL moins intéressant quand N augmente.

% Courbes avec SCL-CRC


% Toutes les simulations en point flottant
% SC Systématique / non systématique
% Différents rendements, tailles de codes, etc...
% Faire le lien avec réseaux mobiles
% 
% SC LIST
% Ne pas trop en faire, développé en chapitre 2
% Différents CRC (retrouver citation des CRC adaptés dans papiers SCL)
% ASCL : montrer pour CRC non adapté (1723/2048 )
% 
% BP    (Thèse guillaume ?)
% SCAN  
% 
% 	
% 
% SC Stack
% SC FLIP
% 
% 

\section{L'élagage de l'arbre de décodage}

% Figure sous-arbre

\subsection{L'élagage de l'algorithme de décodage SC}
Afin de réduire le temps de décodage, il est possible d'élaguer l'arbre de décodage comme indiqué dans \cite{alamdar-yazdi_simplified_2011}. En effet, un sous-arbre dont toutes les feuilles sont de rendement 0 n'a pas besoin d'être parcouru. Comme toutes ses feuilles sont des bits gelés, alors toutes les sommes partielles du sous-arbre sont nulles. Le même élagage est possible pour des sous-arbre dont toutes les feuilles sont de rendement 1. Pour un tel sous-arbre, il suffit d'appliquer la fonction de seuillage \texttt{R1} sur chaque LLR de la racine du sous arbre pour obtenir le même résultat que s'il avait été parcouru.

Deux autres types de sous arbres peuvent être identifiés, et des fonctions spécialisées permettent d'obtenir les valeurs des sommes partielles directement depuis les valeurs des LLRs de la racine du sous-arbre. Soit un sous-arbre dont le \noeud racine a une taille $M$. Si $M-1$ feuilles correspondent à des bits gelés et la dernière à un bit d'information, le sous-arbre peut être remplacé par un \noeud de répétition. Les valeurs des sommes partielles d'un tel \noeud sont soit toutes égales à $0$ soit toutes égales à $1$. Pour déterminer quelles valeurs prennent les sommes partielles, tous les LLRs du \noeud considéré sont additionnés. Si le total est supérieur à 0, alors toutes les sommes partielles sont mises à $0$, et dans le cas contraire, toutes à $1$. Le dernier est un \noeud à test de parité unique (SPC). Plusieurs étapes sont nécessaires au traitement d'un tel \noeud. Ce traitement est noté \texttt{REP} dans la suite du document. Tout d'abord, la fonction élémentaire \texttt{R1} est appliqué sur chaque LLR, comme pour un \noeud de rendement 1. La parité de ce premier vecteur de sommes partielles est testée. Si celle-ci est égale à 0, le test de parité est satisfait, et les sommes partielles ne sont pas modifiées. Dans le cas contraire, les LLRs sont triés selon leur valeur absolue. La somme partielle associée au LLR ayant la valeur absolue la plus faible est inversée. Le vecteur ainsi corrigé respecte cette fois la contrainte de parité.

\subsection{L'élagage de l'algorithme de décodage SCL}

Le mécanisme d'élagage de l'arbre peut être décliné pour l'algorithme SCL, il est toutefois plus complexe à mettre en œuvre. Il faut, pour tous ces \noeud excepté celui de rendement 0, proposer plusieurs candidats. Les mécanisme de traitement de chaque type de \noeud sont énumérés ci-dessous. Pour chaque type de \noeud, la méthode de génération des candidats est exposée, accompagnée du calcul de leurs métriques respectives.

Dans un \noeud de rendement 0, comme pour le SC, les sommes partielles résultantes d'un sous-arbre sont toutes égales à 0. Il n'y a pas de nouveaux candidats à générer. Par contre, les métriques doivent être mises à jour. Le traitement pour chaque LLR en entrée du sous-arbre est le même que pour le traitement d'une feuille de rendement 0. S'il est positif, aucune pénalité n'est appliquée à la métrique du chemin courant. S'il est négatif, une pénalité égale à la valeur absolue du LLR est ajoutée à la métrique. 

Le traitement d'un \noeud de répétition est relativement simple à appréhender. En effet, puisqu'un seul bit d'information est présent dans le sous-arbre, seulement deux versions des sommes partielles en sortie sont possibles, soit toutes à uns, soit toutes à 0. Ces deux versions constituent les deux candidats de chaque chemin. Un \noeud de répétition génère donc deux candidats. A chaque candidat doit être affecté une nouvelle métrique. Cette nouvelle métrique est fonction des LLRs d'entrée du sous-arbre. Pour le candidat \og tout à 0 \fg, la valeur absolue de chaque LLR en entrée du sous-arbre ajoutée en pénalité à la métrique du chemin si et seulement si le LLR est négatif. Pour le \noeud \og tout à 1 \fg, la valeur absolue de chaque LLR en entrée du sous-arbre est ajoutée en pénalité à la métrique du chemin si et seulement si le LLR est positif.

% Ajouter de la citation partout 
% Ajouter citation Chase 2 Sarkis R1
Le traitement du \noeud de rendement 1 est plus intensif en calcul. En effet, si le \noeud est de taille $T$, alors le nombre de candidats possibles est $2^T$. Pour chaque candidat, il faut calculer la nouvelle métrique. Ce calcul est le traitement \texttt{R1} appliqué à chaque LLR en entrée du sous-arbre, soit un seuillage. Comme pour tout \noeud terminal, il faut ensuite trier les $2^T$ candidats afin d'en sélectionner les $L$ ayant la métrique la plus faible. Rapidement, ces traitements deviennent trop intensifs pour une implémentation réaliste. Une diminution drastique de la complexité de ce traitement peut être obtenu par l'utilisation de l'algorithme \og Chase \fg sur le sous-arbre. Tout d'abord, le premier candidat est obtenu en appliquant un seuillage sur les $T$ LLRs d'entrée à la manière du traitement du \noeud de rendement 1 dans l'algorithme SC. Ensuite les $T$ LLRs d'entrée sous tout d'abord triés selon leur valeur absolue. Les deux LLR parmi les $T$  dont la valeur est la plus faible sont identifiés, notés LLR\textsubscript{1} et LLR\textsubscript{2}. Les deuxième et troisième candidats sont générés en inversant la somme partielle associée au LLR\textsubscript{1} et celle associée au LLR\textsubscript{2}. Le quatrième candidat est généré en inversant les deux sommes partielles à la fois. La métrique de chaque candidat est calculée en ajoutant à la métrique du chemin considéré la valeur absolue des LLRs dont la somme partielle a été inversée. Cette méthode empirique fonctionne excellemment. Il est possible que des réductions de performances apparaissent, puisque le nombre de candidats envisagés pour un sous arbre est réduit par rapport à la version non élaguée de l'algorithme SCL. Aucune dégradation n'est observée dans les faits expérimentaux.
 

Enfin, le traitement des \noeuds \texttt{SPC} est similaire au traitement du \noeud de rendement 1 à la différence qu'au lieu de deux bits potentiellement inversés ce sont quatre bits qui sont considérés. De plus, seuls les candidats respectant la contrainte de parité sont générés. A la différence des \noeuds de rendement 1, la performance de correction de l'algorithme élagué peut être dégradé. Les auteurs de \cite{sarkis_fast_2016} signalent en effet que des dégradations importantes sont observées pour des \noeuds \texttt{SPC} de taille supérieure à 4. 


\subsection{L'élagage de l'algorithme de décodage SCAN}
Tout comme les algorithmes SC et SCL, des élagages peuvent être appliqués sur le SCAN. A la différence des deux précédents, les \noeuds \texttt{REP} et \texttt{SPC} ne sont pas élagués. De plus, le concept de sous-arbre dans le SCAN évolue. Tandis que dans l'algorithme SC, un sous arbre prenait comme entrée des LLRs et produisait en sortie des sommes partielles, dans l'algorithme SCAN, les sorties des sous-arbres sont également des LLRs. Dans le cas d'un rendement 0, les sorties du \noeud sont toutes à $+\inf$, puisqu'il est certain que les bits du mots de code sont des $0$. Dans le cas d'un rendement 1, les sorties du \noeud sont tous à $0$ puisqu'en l'absence de redondance, aucune information ne peut être donnée à partir de ce sous arbre.
% détailler calculs évitables (g0, f0, grep)
% discussion chase spc - r1 ? Hashemi-Sarkis + Simulations

% \section{Codes polaires de taille variables}
% https://arxiv.org/pdf/1701.06458.pdf 4-5-6-7-8-9


% \subsection*{Conclusion}
% Dans ce chapitre ont été présentés les codes polaires, avec une emphase particulière sur les différents alogithmes de décodage. Dans les chapitres à venir, plusieurs implémentations de ces algorithmes vont être proposées. Le chapitre suivant porte sur une implémentation logicielle des algorithmes basés sur le SCL. 