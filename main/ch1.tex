%!TEX root = ../my_thesis.tex

\chapter{Les codes polaires}
\label{chap:polar_codes}

Le but de ce premier chapitre est d'introduire le lecteurs au codage polaire, avec une emphase particulière sur les algorithmes de décodage.

Dans la première section, le modèle de la chaîne de communications numériques utilisé tout au long du manuscrit est décrit, puis le principe et la construction des codes polaires sont détaillés. La deuxième section présente les principaux algorithmes de décodage de codes polaires. La troisième section porte sur une simplification algorithmique 




\vspace*{\fill}
\minitocTITI
\vspace*{\fill}

\section{Le principe et la construction des codes polaires}

\subsection{La chaîne de communications numériques}
\label{subsec:contexte}


\begin{figure}[t]
\centering
\includegraphics[width=0.8\textwidth]{main/ch1_fig/chaine_com}
\caption{Modèle simplifié d'une chaîne de communications.}
\label{fig:chaine_com}
\end{figure}
Une chaîne de communications numériques telle que conceptualisée par Claude Shannon \cite{shannon_mathematical_2001} est représentée en Figure~\ref{fig:chaine_com}.
Elle représente les principales étapes de la transmission de données numériques depuis une \textbf{source} vers un \textbf{destinataire} à travers un \textbf{canal} de transmission.
Ce dernier est le support sur lequel transite l'information. Lors de communications sans fil, il s'agit de l'ensemble constitué des antennes d'émission et de réception et de l'espace libre les séparant. Or les grandeurs physiques associées à ce canal sont continues, tandis que les données à transmettre sont constituées de bits, donc discrètes. Le \textbf{modulateur} transforme ce flux binaire en signaux physiques transmissibles par le canal. Si l'on considère toujours les communications sans fils, le modulateur transforme les séquences de bits en formes d'ondes. Au sein du canal de communication, les signaux subissent de nombreuses perturbations comme le bruit thermique des composants électroniques de la chaîne de transmission ou encore les interférences causées par d'autres utilisateurs du canal. Le démodulateur effectue la fonction duale du modulateur, il convertit les signaux physiques en données binaires. Si les perturbations du canal sont trop fortes, il est possible que le modulateur génère une estimation erronée du bit transmis. Ceci est un problème, puisque le but de la chaîne de transmission est de transmettre au destinataire l'exacte réplique de la \textbf{séquence d'information}, $\mathbold{b}$ émise par la source. Lorsque l'information reçue et l'information émise diffèrent, il y a une erreur de transmission. La qualité de la chaîne de transmission est souvent estimée à l'aide de son taux d'erreur binaire (BER), qui correspond à la proportion d'erreurs binaires par rapport au nombre total de bits transmis.

L'ajout d'un encodeur et d'un décodeur de canal dans la chaîne de transmission est un moyen efficace de réduire ce taux d'erreur. Dans le cas des codes en blocs étudiés dans ce manuscrit, l'encodeur transforme une séquence d'information $\mathbold{b}$ de $K$ bits en un \textbf{mot de code} $\mathbold{x}$ de $N$ bits. La taille du mot de code, en nombre de bits, est supérieure à la taille de la séquence d'informations afin d'ajouter de la redondance au message transmis: le rendement du code $R=K/N$ est inférieur à $1$.  Cette redondance est utilisée par le décodeur de canal, ou parfois le couple démodulateur - décodeur canal, afin d'améliorer l'estimation du message transmis et donc de réduire le taux d'erreur binaire de la chaîne de transmission. Dans ce manuscrit, le décodeur de canal est découplé du démodulateur. L'ensemble modulateur - canal - démodulateur peut être considéré comme une seule entité indépendante dont l'entrée est constituée du mot de code $\mathbold{x}$ et la sortie d'une séquence d'estimations de $\mathbold{x}$ notée $\mathbold{L}$. Cet ensemble est appelé canal composite.

\subsection{Le canal composite}
\label{subsec:canal}

Un seul et unique modèle de canal composite sera considéré tout au long de ce manuscrit. Il est constitué d'une modulation à changement de phase binaire (BPSK) qui associe respectivement aux valeurs binaires d'entrée $x\in\{0,1\}$ les valeurs réelles $\tilde{x}\in\{-1,1\}$.

Le canal composite considéré est basé sur un canal à bruit blanc additif gaussien à entrée binaire (BI-AWGNC : Binary Input - Additive White Gaussian Noise Channel) tel que défini dans \cite[section~1.5.1.3]{ryan2009channel}. Ce modèle est classiquement utilisé pour caractériser les performances des codes correcteurs d'erreur. Il consiste en l'addition aux données de sortie du modulateur $\mathbold{\tilde{x}}$ d'une variable aléatoire à valeurs réelles ayant une distribution gaussienne centrée en $0$ et de densité spectrale $N_0$. Afin d'évaluer les performances de correction des codes correcteurs d'erreurs, les taux d'erreurs seront souvent rapportés à cette densité spectrale, ou plus précisément au \textbf{rapport signal à bruit} (SNR), noté $E_b/N_0$, où $E_b=\frac{\mathbb{E}(\hat{x}^2)}{R}$ est l'énergie moyenne par bit d'information.

Les estimations en sortie du démodulateur sont données sous la forme de rapport de vraisemblance logarithmique (LLR : Logarithmical Likelihood Ratio). Leur signe détermine pour chaque donnée de sortie du canal $\tilde{y}_i \in \mathbold{\tilde{y}}$ la valeur binaire d'entrée $x_i \in \mathbold{x}$ la plus probable. La valeur absolue correspond au degré de fiabilité de l'information. Plus de détails sur le modèle de canal et la définition des LLR sont donnés en Annexe \ref{append:decoding_nodes}. L'expression mathématique de $L_i$ est la suivante : 
\begin{equation*}
  L_i = \log\left(\dfrac{P_r(y_i | x_i = 0)}{P_r(y_i | x_i = 1)}\right)
\end{equation*}
\subsection{Les codes polaires}
Les codes polaires \cite{arikan_channel_2009} sont des codes en blocs linéaires \cite{morelos-zaragoza_art_2006}. Soient les ensembles $\mathbb{B}^n = \{0,1\}^n$, et une matrice $G \in \mathbb{B}^K \times \mathbb{B}^N$, l'encodage d'un code en blocs linéaire est une application linéaire injective de $\mathbb{B}^K$ vers $\mathbb{B}^N$ qui à un élément $\mathbold{b} \in \mathbb{B}^K$ associe un élément $\mathbold{x} \in \mathbb{B}^N$ tel que $x=\mathbold{b}G$. Les codes polaires peuvent ainsi être définis comme un ensemble de codes en blocs linéaires ayant une matrice génératrice avec une forme particulière.

La matrice génératrice d'un code polaire est elle-même le produit de deux matrices $E \in (\mathbb{B}^K \times \mathbb{B}^N)$ et $F^{\otimes n}\in (\mathbb{B}^N)^2$ tel que $G=EF^{\otimes n}$. La multiplication par la matrice $E$ correspond à l'introduction de "bits gelés", dont la valeur est $0$, à la séquence d'information $\mb{b}$. Le résultat de l'opération $\mathbold{u} = E\mathbold{b}$ est constitué de $K$ bits $b_i \in \mathbold{b}$ et de $N-K$ bits gelés. Les positions respectives des bits d'informations et des bits gelés sont liées au phénomène de polarisation décrit dans la sous-section suivante. Ces positions déterminent largement le pouvoir de correction du code.

La matrice $F^{\otimes n}$, où $N=2^n$, est la n-ième puissance de Kronecker du noyau $F=\left[\begin{smallmatrix} 1 & 0 \\ 1 & 1\end{smallmatrix}\right]$. $F^{\otimes n}$ peut être définie récursivement : $F^{\otimes 1} = F$ et $\forall {n \geq 1}\text{ , }{F^{\otimes n + 1}=\left[\begin{smallmatrix} F^{\otimes n} & 0_n \\ F^{\otimes n} & F^{\otimes n}\end{smallmatrix}\right]}$ avec $0_n \in \mathbb{B}^N$ une matrice nulle. La représentation sous forme de graphes de factorisation (\textit{factor graph}), dans la figure \ref{fig:encodage}, est une représentation graphique du processus d'encodage de codes polaires où les symboles $\oplus$ correspondent à des opérations \textit{ou-exclusif}. 

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{main/ch1_fig/Graph_N_rec}
\caption{Représentation en graphe de factorisation de l'encodage de codes polaires.}
\label{fig:encodage}
\end{figure}

Le processus d'encodage peut également être modifié pour rendre le code systématique. Un code correcteur d'erreur est dit systématique si les bits de la séquence d'information sont présents dans le mot de code. Pour ce faire, il faut modifier la matrice d'encodage comme démontré dans \cite{arikan_systematic_2011} : $G_{sys}=EF^{\otimes n}EF^{\otimes n}$. La représentation en graphe de factorisation de ce type de matrice est donnée en Figure \ref{fig:sys}. Lorsque cette matrice d'encodage est utilisée, les bits de la séquence d'information $\mb{b}$ se trouvent dans le mot de code $\mb{x}$, car si $\mb{x}=\mb{b}G_{sys}$ alors $\mb{b}=E^{-1}\mb{x}$ .

\begin{figure}[t]
\centering
\includegraphics[width=0.7\textwidth]{main/ch1_fig/Graph_N_sys}
\caption{Encodage systématique.}
\label{fig:sys}
\end{figure}

\subsection{Le phénomène de polarisation}
\label{subsec:polarisation}
La spécificité des codes polaires est liée au phénomène de polarisation. L'ensemble constitué de l'encodeur polaire, du canal composite et du décodeur polaire peut être modélisé comme un ensemble de $N$ canaux de transmission. Chacun de ces canaux transmet un bit. Le terme \og polarisation \fg exprime le fait que ces $N$ canaux ont tendance à se diviser en deux groupes. Un groupe de canaux très fiables, avec une probabilité d'erreur très faible, et un groupe de canaux peu fiables, à probabilité d'erreur forte.
Il est possible de montrer que la proportion de canaux très fiables, pour un rendement de code donné, tend vers la capacité du canal telle que définie dans la théorie de l'information \cite{shannon_mathematical_2001} lorsque la taille du mot de code $N$ tend vers l'infini.
Lors de l'étape d'encodage, les bits d'information sont attribués aux canaux fiables et les bits dits gelés, dont la valeur est connue a priori, sont attribués aux canaux peu fiables. La transformation en produit de Kronecker étant commune à tous les codes polaires, la construction d'un code polaire consiste à déterminer la position des bits gelés dans le vecteur $\mathbold{u}$ pour une taille de code donnée. La méthode de détermination des positions des bits gelés est donnée dans \cite{arikan_channel_2009} pour les canaux à effacement. Pour le canal AWGN, les premières tentatives de construction de code polaire furent décrites dans \cite{mori_performance_2009,mori_properties_2010}, mais les complexités calculatoires associées à ces méthodes sont significatives. Des méthodes simplifiées, utilisées dans la pratique, ont été proposées dans \cite{tal_how_2013,trifonov_efficient_2012,trifonov_randomized_2018} pour le canal AWGN. La valeur des bits gelés, dans la plupart des cas, est 0. Toutefois, des travaux récents \cite{trifonov_polar_2016,trifonov_randomized_2017} montrent que de meilleures performances de décodage peuvent être atteintes en assignant des valeurs dynamiques à ces bits gelés. Ces valeurs dynamiques sont des combinaisons des valeurs des bits précédents. Par exemple, si $u_j$ est un bit gelé, sa valeur peut être définie de la manière suivante : $u_j=\sum^{j-1}_{i=0} \alpha_i u_i$, avec $\alpha_i \in \{0,1\}$. Dans l'ensemble des simulations et des explications contenues dans ce manuscrit, il est cependant considéré que la valeur des bits gelés est 0.

\section{Les algorithmes de décodage de codes polaires}

Le phénomène de polarisation et les codes polaires ont été développés en supposant l'utilisation de l'algorithme de décodage par Annulation Successive (SC : Successive Cancellation). Il s'agit d'un algorithme à sortie dure : les données de sortie sont des bits. Toutefois un deuxième algorithme de décodage par propagation de croyance (BP : Belief Propagation) fut également proposé. Celui-ci est à sortie souple. Cela signifie que les données de sortie sont des estimations représentées typiquement sous forme de LLR. Pour rappel, dans la représentation en LLR, le signe correspond à la décision dure : bit égal à 0 pour un LLR positif, bit égal à 1 pour un LLR négatif. 
La valeur absolue informe sur la fiabilité de cette décision. Bien que l'algorithme SC soit optimal asymptotiquement lorsque $N$ tend vers l'infini, ses performances sont limitées pour des codes de taille finie. De nombreuses variantes de ces algorithmes ont donc été proposées par la suite afin d'en améliorer les performances de correction. Les algorithmes Annulation Successive par Liste (SCL) \cite{tal_list_2011}, Annulation Successive "Flip" (SCF) \cite{afisiadis_low-complexity_2014}, Annulation Successive par Pile (SCS) \cite{niu_stack_2012} sont des évolutions de l'algorithme SC. Par ailleurs, l'algorithme appelé Annulation Souple (SCAN) \cite{fayyaz_low-complexity_2014} peut être vu comme une combinaison des algorithmes SC et BP. Chacun de ces algorithmes est décrit dans cette section.

\subsection{L'algorithme de décodage par Annulation Successive (SC)}

% Pas convaincu du tout, voir avec Camille
\subsubsection{Le décodage par Annulation Successive sur le graphe de factorisation}

\begin{figure}[t]
  % \renewcommand*\thesubfigure{\arabic{subfigure}} 
  \centering
  \subfloat[][Channel LLR.~~~~~~~~~~~~]
  {
  \includegraphics[scale=1.3]{main/ch1_fig/sc_graph_1}
    \label{fig:sc_steps_1}
  }\hspace*{0.5cm}
  \subfloat[][Fonction $f$.]{\includegraphics[scale=1.3]{main/ch1_fig/sc_graph_2}}\hspace*{0.5cm}
  \subfloat[][Fonction $\texttt{R0}$.]{\includegraphics[scale=1.3]{main/ch1_fig/sc_graph_3}}\\
  \subfloat[][Fonction $g$.]{\includegraphics[scale=1.3]{main/ch1_fig/sc_graph_4}}\hspace*{1.55cm}
  \subfloat[][Fonction $\texttt{R1}$.]{\includegraphics[scale=1.3]{main/ch1_fig/sc_graph_5}}\hspace*{0.8cm}
  \subfloat[][Fonction $h$.]{\includegraphics[scale=1.3]{main/ch1_fig/sc_graph_6}}
  \caption{Fonctions élémentaires et séquencement du décodage SC d'un noyau $N=2$.}
  \label{fig:sc_steps}
\end{figure}

% Expliciter notation (N,K)
% Mettre cohérence dans numérotation des niveaux (0 = racine)

Le décodage peut être représenté sous forme de graphe de factorisation. Lors de l'encodage, représenté en Figure~\ref{fig:encodage}, les bits d'information et les bits gelés sont à gauche du graphe et le parcours de gauche à droite de celui-ci permet de calculer les bits du mot de code. Lors du décodage, opération duale de l'encodage, le sens du parcours est inversé. Les informations du canal viennent de la droite (Figure~\ref{fig:sc_steps_1}). Elles représentent les estimations des bits du mot de code. 
Les étapes du décodage du noyau élémentaire de taille 2 sont détaillées en Figure~\ref{fig:sc_steps}.

Les données d'entrée des algorithmes de décodage présentés ici sont donc des LLR, contenus dans le vecteur $\mathbold{L}$ de taille $N$, sortis du canal composite. Les données de sorties de l'algorithme de décodage sont quant à elles des bits contenus dans le vecteur $\mathbold{\hat{b}}$ de taille $K$.
Durant le processus de décodage de codes polaires, les deux formats de données,  les LLR d'une part, et les sommes partielles (PS : Partial Sums) d'autre part, sont utilisés. 
Les LLR notés $L_{i,j}$ sont les estimations de la valeur du bit à la position $(i,j)$ du graphe de factorisation. Les sommes partielles notées $s_{i,j}$ correspondent à la décision dure de ce même bit du graphe de factorisation.
Huit variables sont nécessaires pour le décodage du noyau élémentaire de taille 2, 4 LLR et 4 sommes partielles.

Ces variables apparaissent dans la Figure \ref{fig:sc_steps} représentant les différentes étapes de l'algorithme de décodage SC.
La première étape (a) du décodage est le chargement des LLR du canal $L$ : les LLR $L_{0,0}$ et $L_{1,0}$ prennent la valeur des LLR du canal. Les LLR et les sommes partielles de chaque point du graphe de factorisation sont ensuite calculés par l'intermédiaire des opérations $f$, \texttt{R0}, $g$, \texttt{R1} et $h$ symbolisées dans la Figure~\ref{fig:sc_steps} par des flèches. Elles correspondent aux équations suivantes :
\begin{eqnarray}
  \begin{array}{l c l}
    f(L_a,L_b) &=& \text{sign}(L_a.L_b).\min(|L_a|,|L_b|)\\
    g(L_a,L_b,\hat{s}_a)&=&(1-2\hat{s}_a)L_a+L_b\\
    h(\hat{s}_a,\hat{s}_b)&=& (\hat{s}_{a} \oplus \hat{s}_{b}, \hat{s}_{b})\\
    \texttt{R0}(L_a) &=& 0 \\
    \texttt{R1}(L_a) &=&  \left\{\begin{array}{l c l} 0 \text{ si } L_a \geq 0 \\ 1 \text{ si } L_a < 0 \end{array}\right.
  \end{array}
  \label{eq:sc}
\end{eqnarray}
La fonction $f$ est appliquée dans l'étape (b). Elle permet le calcul de $L_{0,1}$.
L'étape (c) est l'application de la fonction \texttt{R0} sur le \noeud supérieur. 
La fonction \texttt{R0} est appliquée dans l'hypothèse où le bit $u_0$ (Figure \ref{fig:encodage}) est un bit gelé.
La fonction $g$ permet ensuite, lors de l'étape (d), le calcul de $L_{1,1}$.
L'étape (e) est le calcul de $s_{1,1}$ par l'opération \texttt{R1} correspondant à un bit d'information. 
Cette opération est un seuillage du LLR pour obtenir la somme partielle.
Lorsque le seuillage est appliqué, l'information de fiabilité est perdue.
Après le décodage de la somme partielle $s_{1,1}$, la fonction $h$ est appliquée afin de propager les sommes partielles, lors de l'étape (f).


\subsubsection{Le décodage par Annulation Successive sur un arbre binaire}
\begin{figure}[t]
\centering
\includegraphics[width=0.55\textwidth]{main/ch1_fig/sc}
\caption{Arbre de décodage SC.}
\label{fig:sc}
\end{figure}
% mettre en évidence qu'on décode bit par bit, sera utile pour sc list
% introduire termes rendement 1 / rendement 0
% sommes partielles termes non introduit
% vérifier que n est introduit
La représentation en arbre, comme donnée en Figure~\ref{fig:sc}, est une représentation alternative qui permet de mieux représenter certaines simplifications algorithmiques.
Les données sont organisées en arbre binaire sur $log_2(N) + 1$ \textbf{niveaux}. Dans notre exemple pédagogique, l'arbre possède quatre niveaux.
\`A chaque niveau, un certain nombre de \textbf{\noeuds} sont attribués.
La \textbf{racine} de l'arbre numérotée $0$ est constituée d'un seul \noeud.
La racine contient $N$ LLR et $N$ sommes partielles.
En parcourant l'arbre, à chaque niveau, le nombre de \noeuds double.
\`A l'inverse, les nombres de LLR et de sommes partielles de chaque \noeud sont divisés par deux.
Les \textbf{feuilles} sont les \noeuds du dernier niveau de l'arbre (niveau 3 dans la Figure~\ref{fig:sc}).
Le traitement d'une feuille correspond à l'application des opérations \texttt{R0} et \texttt{R1}.
Chaque niveau $d$ contient $2^d$ \noeuds constitués de $2^{n-d}$ LLR et de $2^{n-d}$ sommes partielles, où $n=\log_2(N)$. 
Les sommes partielles sont propagées par la fonction $h$. Elles sont propagées jusqu'à un niveau qui dépend de l'indice de la feuille qui vient d'être traitée.

Dans le cas d'un code polaire non systématique, la séquence décodée $\hat{u}$ est composée des sommes partielles contenues par les feuilles de l'arbre de décodage.
Lors d'un décodage non systématique, les sommes partielles contenues par les feuilles correspondent à la séquence décodée $\hat{u}$.
La Figure~\ref{fig:seq_sc} est une représentation plus synthétique de l'arbre de décodage.
Les opérations 12 et 13 de la Figure \ref{fig:seq_sc} sont donc inutiles puisqu'elles servent seulement à calculer les sommes partielles du \noeud racine.
Dans le cas d'un code systématique, ce sont les sommes partielles du \noeud racine qui correspondent au mot de code décodé.
Le parcours de l'arbre est un parcours en profondeur.
L'ordonnancement des différentes fonctions est explicité dans la Figure~\ref{fig:seq_sc}, où chaque flèche représentant une fonction est numérotée selon l'ordre d'exécution.


\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{main/ch1_fig/seq_sc}
\caption{Ordonnancement du décodage SC d'un code polaire de taille $N=4$.}
\label{fig:seq_sc}
\end{figure}

\subsubsection{Les niveaux de parallélisme de l'algorithme  de décodage SC}
\label{subsubsec:parallel}
Dans la Figure~\ref{fig:sc}, il apparaît que quatre fonctions $f$ sont appliquées sur le \noeud racine pour calculer les LLR de la branche de gauche. Ces quatre fonctions sont indépendantes : leurs entrées et sorties sont disjointes. Elles peuvent donc être réalisées simultanément. Ce parallélisme existe pour chaque fonction $f$, $g$ et $h$ d'un \noeud donné. Ainsi, le niveau de parallélisme est différent selon le niveau de profondeur dans l'arbre de décodage SC. Le niveau de parallélisme correspond à la taille du \noeud cible. Plus on se rapproche des feuilles, plus la taille des \noeuds est faible et, donc, plus le parallélisme est faible. Dans les implémentations logicielles ou matérielles de ces algorithmes, ce parallélisme est utilisé pour réduire le temps de décodage et ainsi augmenter son débit. Lorsque ce parallélisme est utilisé, on parle de parallélisme \textit{intra-trame}. Un autre type de parallélisme existe et est appelé parallélisme \textit{inter-trame}. Il désigne la possibilité, dans les implémentations de décodeurs, de traiter plusieurs trames simultanément. Il possède le désavantage, contrairement au parallélisme \textit{intra-trame}, d'augmenter la latence de décodage. Généralement, la sélection de l'un ou l'autre des types de parallélisme correspond à un compromis entre le débit et la latence de décodage.


\subsection{L'algorithme de décodage SC Liste}
\subsubsection{L'algorithme de décodage}
L'algorithme de décodage par Annulation Successive Liste (SCL) est une évolution de l'algorithme SC \cite{tal_list_2011}. L'algorithme SC présente en effet des performances de correction médiocres pour des codes polaires de petite ou moyenne taille ($N < 8192$). L'algorithme de décodage SCL améliore ces performances substantiellement. Dans l'algorithme de décodage SC décrit précédemment, des décisions dures sont appliquées à chaque traitement d'une feuille correspondant à un bit d'information (fonction \texttt{R1}). Si une erreur est obtenue lors de ce seuillage, celle-ci est irréversible. Cela signifie que le décodage du mot de code est un échec. Le principe de l'algorithme de décodage par liste est de retarder la décision dure. Au lieu d'appliquer un seuillage sur la valeur du LLR, les deux possibilités de décodage sont considérées. Pour ce faire, l'arbre de décodage, avec l'ensemble de ses LLR et sommes partielles, est dupliqué. Cette opération est illustrée dans la Figure~\ref{fig:scl}.

L'algorithme est représenté en cours d'exécution, à l'étape (i), au cours de laquelle la fonction $f$ est appliquée. L'étape suivante (i+1) consiste en la duplication de l'arbre de décodage afin de former deux chemins. Dans le premier, l'hypothèse est que $\hat{u}_2=0$. Dans le second, l'hypothèse est que $\hat{u}_2=1$. Le décodage se poursuit en parallèle sur les deux arbres. L'étape suivante (i+2) correspond à l'application de la fonction $g$. Les deux arbres sont qualifiés de \textbf{chemins} de décodage. \`A chaque feuille de rendement 1, le même procédé est appliqué, doublant le nombre d'arbres à décoder en parallèle comme durant l'étape (i+3).

\begin{figure}[t]
\centering
\includegraphics[width=1\textwidth]{main/ch1_fig/scl}
\caption{Duplication de l'arbre de décodage dans l'algorithme SCL.}
\label{fig:scl}
\end{figure}

Il n'est pas possible, en pratique, de dupliquer indéfiniment le nombre d'arbres de décodage, la mémoire nécessaire et le nombre de calcul devient vite insoutenable. Un nombre de chemins maximum $L$ est donc un paramètre de l'algorithme liste. La sélection des chemins qui seront conservés ou éliminés est réalisée à l'aide d'une \textbf{métrique} $m^i_j$ associée à chaque chemin. Cette métrique est mise à jour à chaque traitement d'une feuille de l'arbre. Lors des autres étapes, par exemple l'application de la fonction $g$ en étape (i+2), les métriques sont inchangées, $m^{i+2}_j = m^{i+1}_j$. Le détail des calculs de métriques lors du traitement d'une feuille pour un décodage utilisant des LLR est donné dans \cite{balatsoukas-stimming_llr-based_2015}. Deux cas de figure sont possibles : soit la feuille correspond à un bit gelé, soit elle correspond à un bit d'information.

Lorsque la feuille correspond à un bit gelé, cela veut dire que sa somme partielle est égale à 0. Lorsque le LLR de cette feuille est calculé, il y a deux cas de figure. Ou bien celui-ci est positif, auquel cas la métrique est inchangée, car l'estimation du LLR est juste. 
En revanche, si le LLR est négatif, alors la valeur absolue du LLR lui est ajoutée. On peut interpréter cette addition comme une \og pénalité \fg pour avoir fait une mauvaise estimation. Soit $i_0$ l'indice correspondant à une étape de traitement d'une feuille de rendement 0, et $L_j^{i_0}$ la valeur de son LLR, alors, pour un chemin donné d'indice $j$, 
\begin{equation*}
m^{i_0}_j=\left\{\begin{array}{l c l} m_j^{{i_0}-1} & \text{ si } L_j^{i_0} \geq 0 \\ m_j^{{i_0}-1} + |L_j^{i_0}| & \text{ si } L_j^{i_0} < 0 \end{array}\right.
\end{equation*}

Lorsque la feuille est de rendement 1, deux chemins sont créés à partir de chaque chemin actif, avec chacun une version différente du bit décodé. Le chemin créé dont la valeur du bit est en accord avec la valeur du LLR ne reçoit pas de pénalité. Au contraire, la métrique du second chemin créé est augmentée de la valeur du LLR. Soit $i_1$ l'étape de décodage d'une feuille de rendement 1,
\begin{equation*}
m^{i_1}_j=\left\{\begin{array}{l c l} m_j^{{i_1}-1}               & \text{ si } L^{i_1}_j \geq 0 \text{ et } \hat{u}^{i_1}_j = 0
                                   \\ m_j^{{i_1}-1} + |L^j_{i_1}| & \text{ si } L^{i_1}_j < 0    \text{ et } \hat{u}^{i_1}_j = 0
                                   \\ m_j^{{i_1}-1} + |L^j_{i_1}| & \text{ si } L^{i_1}_j \geq 0 \text{ et } \hat{u}^{i_1}_j = 1
                                   \\ m_j^{{i_1}-1}               & \text{ si } L^{i_1}_j < 0    \text{ et } \hat{u}^{i_1}_j = 1

                \end{array}\right.
\end{equation*}

Une fois les calculs de métriques réalisés, les chemins à conserver sont sélectionnés. Si $L$ chemins étaient actifs au moment du traitement d'une feuille de rendement 1, alors $2L$ candidats sont générés, avec chacun une métrique. Seuls les $L$ candidats avec la métrique la plus faible sont conservés. Cette opération de sélection est décrite dans la Figure~\ref{fig:scl}. Lors de la dernière étape, à droite de la figure, les deux candidats du bas sont éliminés. L'hypothèse était donc que ces deux chemins avaient des métriques de valeurs absolues supérieures à celles des deux candidats du haut. \`A la fin du parcours de l'arbre de décodage, $L$ chemins seront donc conservés. Ces $L$ chemins correspondent à $L$ mots de codes candidats. Une sélection d'un seul candidat parmi les $L$ doit alors être effectuée. Dans la version originale de l'algorithme, le mot décodé est celui ayant la métrique la plus faible à la fin du décodage. Il est désigné comme étant la sortie de l'algorithme $\mathbold{\hat{b}}$.

% Insistier lourdement sur la signification de chaque terme : chemin, métrique, candidat

\subsubsection{Concaténation avec un CRC}

Les auteurs de \cite{tal_how_2013} ont proposé de concaténer un test de redondance cyclique (CRC : Cyclic Redundancy Check) pour discriminer les différents candidats. L'encodage correspondant est représenté en Figure~\ref{fig:crc}. Un CRC est un code détecteur d'erreur. Il associe à une séquence binaire quelconque, ici $\mathbold{b}$ de longueur $K$, une séquence de longueur $K+c$. Après le décodage, la vérification du CRC permet de détecter de potentielles erreurs. Le CRC est construit de manière à ce que la probabilité d'erreur non détectée soit extrêmement faible.
Ainsi, à la fin de l'algorithme de décodage, lorsque les $L$ mots de codes candidats en sortie de l'algorithme SCL sont disponibles, une vérification des CRC est effectuée. Si l'un d'entre eux vérifie le CRC, il est alors hautement probable qu'il soit le bon candidat. Dans le cas rare où plusieurs candidats vérifient le CRC, celui ayant la métrique la plus faible parmi eux est sélectionné. Lorsque ce mécanisme de sélection des candidats par la vérification du CRC est appliqué, l'algorithme est appelé \og SCL aidé d'un CRC \fg (CASCL : CRC-Aided SCL).

\begin{figure}[t]
\centering
\includegraphics[width=0.6\textwidth]{main/ch1_fig/crc}
\caption{Concaténation avec un CRC.}
\label{fig:crc}
\end{figure}
\subsubsection{Les algorithmes de décodage SCL adaptatifs}
\label{subsubsec:adaptive}

L'algorithme CASCL permet d'améliorer significativement les performances de décodage en comparaison de l'algorithme SC.
Cette amélioration se fait toutefois au prix d'une augmentation de la complexité calculatoire.
Cependant, il est possible de combiner les deux algorithmes, SC et CASCL, afin de se rapprocher de la faible complexité calculatoire du premier tout en conservant les performances de décodage du second \cite{li_adaptive_2012}.
Le principe est le suivant : une première tentative de décodage est réalisée avec l'algorithme SC. 
L'algorithme CASCL n'est ensuite déclenché que si une erreur de décodage est détectée par la vérification du CRC.

Deux variantes de l'algorithme adaptatifs sont possibles.
Dans le cas de l'algorithme Partiellement Adaptatif (PASCL) présenté dans \cite{sarkis_fast_2016}, lors de l'échec du décodage SC, l'algorithme CASCL est exécuté une seule fois avec comme taille de liste $L_{max}$.
Dans le cas de l'algorithme Complètement Adaptatif (FASCL) original \cite{li_adaptive_2012}, l'algorithme CASCL peut être appliqué plusieurs fois.
La première tentative de décodage CASCL est réalisée avec , $L=2$. Si après décodage, le CRC n'est toujours pas satisfait, l'algorithme est relancé autant de fois que nécessaire en doublant itérativement $L$ jusqu'à atteindre $L_{max}$.



Ces algorithmes permettent de favoriser des architectures de décodage atteignant de hauts débits. Cependant, ces algorithmes ont également une latence maximum plus grande par rapport à l'algorithme CASCL avec $L=L_{max}$. Soit cette latence, $\mathcal{L}_{SCL}(L_{max})$, et la latence de l'algorithme SC, $\mathcal{L}_{SC}$, alors dans le Pire Cas (PC),
\begin{equation*}
\left\{\begin{array}{l c l}
\mathcal{L}^{PC}_{PASCL} & = &\mathcal{L}_{SC} + \mathcal{L}_{SCL}(L_{max})\\
\mathcal{L}^{PC}_{FASCL} & = &\mathcal{L}_{SC} + \displaystyle\sum_{i=1}^{\log_2(L_{max})}\mathcal{L}_{SCL}(2^i)
 \end{array}\right.
\end{equation*}
\subsection{L'algorithme de décodage SC \textit{Flip}}

	L'algorithme de décodage par Annulation Successive \og Flip \fg (SCF : Successive Cancellation Flip) \cite{afisiadis_low-complexity_2014} est une variante de l'algorithme SC. Il vise à améliorer les performances de décodage. Cet algorithme nécessite une concaténation du code polaire avec un CRC pour être appliqué. L'algorithme de décodage SC est appliqué une première fois. L'ensemble des LLR associés aux décisions dures (les LLR des feuilles) est conservé. Si le mot de code décodé satisfait le CRC, alors le décodage s'arrête. Par contre, si le CRC n'est pas satisfait, alors l'algorithme SC est exécuté une nouvelle fois. La différence est que la décision prise, lors du premier décodage, sur le LLR le moins fiable est inversée. Ce deuxième mot décodé est encore testé à l'aide du CRC. S'il ne satisfait toujours pas le CRC, une autre séquence de décodage est lancée en inversant le bit du deuxième LLR le moins fiable. Ce mécanisme itératif continue jusqu'à un nombre d'essais maximum $T$.


	\subsection{L'algorithme de décodage SC \textit{Stack}}
	L'algorithme de décodage par Annulation Successive à Pile (SCS : Successive Cancellation Stack) \cite{niu_stack_2012} est une variante de l'algorithme SCL. Dans l'algorithme SCL, le nombre de candidats $L$ est constant tout au long de l'algorithme (hormis lors des premières duplications). De plus, le séquencement est exactement le même que pour l'algorithme SC dans les différents arbres décodés parallèlement. Les feuilles sont décodées l'une après l'autre, suivant l'ordre établi. Dans l'algorithme SCS, les métriques associées aux différents arbres de décodage sont maintenues dans une pile ordonnée de profondeur $T$. L'algorithme SC est appliqué sur le chemin associé à la meilleure métrique. A l'arrivée sur une feuille, la métrique de l'arbre est mise à jour. Si la feuille est de rendement 1, un nouveau chemin est ajouté à la pile. \`A un instant $t$, seul l'arbre étant le plus haut sur la pile est décodé. Lorsque la dernière feuille de l'un des chemins est décodée, le CRC est testé. Le décodage se termine lorsqu'un mot décodé satisfait le CRC ou que le nombre de candidats testés atteint une valeur déterminée à l'avance $D$. L'algorithme SCS présente l'avantage de nécessiter moins d'opérations élémentaires pour le décodage des codes polaires. Cependant, le maintien d'une pile et son ordonnancement particulier rendent difficile son implémentation.
  % Citer Harsch
	% Parler du Stack Hybride ? 

\subsection{Les algorithmes itératifs à sortie souple}
\label{subsec:soft_algo}
% Parler concaténation

Deux algorithmes itératifs à sortie souple ont été proposés. Le premier est appelé algorithme à Propagation de Croyance (BP). C'est le premier dans l'ordre chronologique puisqu'il fut proposé dans l'article original d'Ar{\i}kan \cite{arikan_channel_2009}. Le second fut proposé en 2014 dans \cite{fayyaz_low-complexity_2014}. Les deux algorithmes ne diffèrent que par l'ordre dans lequel les LLR du graphe de factorisation sont mis à jour durant chaque itération.

\begin{figure}[t]
  \renewcommand*\thesubfigure{\arabic{subfigure}} 
  \centering
  \subfloat[][Initialisation.]{\includegraphics[scale=1.2]{main/ch1_fig/scan_graph_1}}\hspace{2.5cm}
  \subfloat[][Fonction $f_{bp}.$]{\includegraphics[scale=1.2]{main/ch1_fig/scan_graph_2}}\\
  \hspace{0.5cm}
  \subfloat[][Fonction $g_{bp}.$]{\includegraphics[scale=1.2]{main/ch1_fig/scan_graph_3}}\hspace{3.7cm}
  \subfloat[][Fonction en retour.]{\includegraphics[scale=1.2]{main/ch1_fig/scan_graph_4}}
  \caption{Fonctions élémentaires et séquencement du décodage SCAN du noyau $N=2$.}
  \label{fig:SCANSchedule}
\end{figure}
La première différence entre ces deux algorithmes et ceux vues précédemment est qu'aucune décision dure n'est faite durant le décodage. L'information reste souple de bout en bout.
En conséquence, les sommes partielles utilisées dans l'arbre de décodage SC sont remplacées par des LLR. Deux ensembles de LLR sont donc contenus dans l'arbre de décodage. Les LLR qui ont remplacé les sommes partielles sont notés $L^b_{i,j}$.
Ensuite, les fonctions élémentaires sont changées. les fonctions $f$, $g$ et $h$ de l'algorithme SC sont remplacées par les fonctions $f_{bp}$ et $g_{bp}$ :
\begin{eqnarray}
  \begin{array}{l c l}
    f_{bp}(L_a,L_b,L_c) & = & f(L_a, L_b  + L_c) \\
    g_{bp}(L_a,L_b,L_c) & = & f(L_a, L_c) + L_b
  \end{array}
  \label{eq:bp}
\end{eqnarray}
Le séquencement de ces opérations est illustré en figure \ref{fig:SCANSchedule} pour le noyau de taille $N=2$ dans le cas d'un décodage SCAN. La première étape est l'initialisation de la racine et des feuilles de l'arbre. Les LLR $L$ de la racine prennent la valeur des LLR du canal. Les LLR $L^b$ des feuilles prennent la valeur $+\infty$ dans le cas d'un bit gelé. et la valeur $0$ dans le cas d'un bit d'information. Ensuite, les fonctions $f_{bp}$ et $g_{bp}$ sont appliquées dans l'ordre indiqué.
Comme évoqué précédemment, les algorithmes SCAN et BP diffèrent du point de vue de leurs séquencements. Le séquencement de l'algorithme SCAN est le même que celui du SC, en remplaçant l'application de la fonction $h$ du SC par une application conjointe des fonctions $f_{bp}$ et $g_{bp}$ comme illustré en Figure~\ref{fig:SCANSchedule} (4).

Enfin, les algorithmes BP et SCAN sont des algorithmes itératifs. En effet, le parcours de l'arbre peut être effectué plusieurs fois. Ainsi, les valeurs des LLR intermédiaires évoluent d'une itération à l'autre. Ces itérations permettent d'améliorer les performances de décodage.
Par ailleurs, l'intérêt des algorithmes de décodage à sorties souples est qu'ils peuvent être utilisés dans un processus itératif si le code polaire est concaténé avec un autre code \cite{balatsoukas-stimming_polar_2017} ou bien dans le cadre d'une modulation codée \cite{dai_polar_2016}.

\subsection{Les performances de décodage des différents algorithmes}
Le modèle du canal composite utilisé est celui décrit dans la sous-section \ref{subsec:canal}. La méthode d'approximation gaussienne est utilisée pour déterminer les positions des bits gelés. 
Dans la sous-section courante sont présentées plusieurs courbes de performances. 
Elles permettent de comparer entre eux les algorithmes décrits précédemment.
Ces courbes montrent également l'impact des paramètres, communs à tous les algorithmes, ou spécifiques à chacun, sur les performances de décodage.
Ces paramètres et leurs effets sont importants car ce sont les leviers utilisés dans les standards de communication afin de s'adapter aux contraintes systèmes.
Les axes des différentes courbes sont toujours les mêmes.
En abscisse se trouve le rapport signal à bruit noté $E_b/N_0$ décrit en sous-section \ref{subsec:canal}.
Sur l'axe des ordonnées se trouve soit le taux d'erreur binaire (BER : Bit Error Rate) dont l'expression est donnée en sous-section \ref{subsec:contexte}, soit le taux d'erreur trame (FER : Frame Error Rate). Le FER est égal au rapport du nombre de trames erronées sur le nombre total de trames émises, sachant qu'une trame est considérée erronée si elle contient au moins un bit erroné. Sauf indication contraire, toutes les simulations ont été réalisées en utilisant le logiciel AFF3CT\footnote{AFF3CT est un logiciel libre permettant la simulation efficace de codes correcteurs d'erreurs : https://aff3ct.github.io/} développé par l'équipe CSN du laboratoire IMS.
  
\subsubsection{Impact de la taille du mots de code et du rendement}

La théorie de l'information énoncée par Shannon \cite{shannon_mathematical_2001} prouve qu'il existe une limite théorique à la capacité d'un canal pour un rendement de code donné.
Cette limite est matérialisée dans la Figure \ref{fig:sc_n} par la ligne verticale dénommée \textit{Limite de Shannon}. Les codes polaires sont les premiers codes correcteurs d'erreurs qui atteignent théoriquement cette limite. Cependant cette limite n'est atteinte que pour une taille de mot de code infinie.

La Figure~\ref{fig:sc_n} présente les performances de décodage de l'algorithme SC pour une taille de mot code croissante. S'il apparaît clairement que le taux d'erreur binaire diminue pour un rapport signal à bruit donné, la limite de Shannon est toutefois très éloignée pour les tailles de mot de code présentées. 

\begin{figure}[h]
  \centering
\input{main/ch1_fig/curves/sc_N/tikz/source.tex}
\caption{Performances de décodage de l'algorithme SC pour différentes valeurs de $N$ ($R=1/2$).}
\label{fig:sc_n}
\end{figure}

\begin{figure}[h]
  \centering
\input{main/ch1_fig/curves/sc_R/tikz/source.tex}
\caption{Performances de décodage de l'algorithme SC pour différentes valeurs de $R$ ($N=2048$).}
\label{fig:sc_r}
\end{figure}
\clearpage
Les codes correcteurs d'erreurs reposent la notion de redondance, quantifiée par le rendement du code. Plus le rendement est faible, plus la redondance est élevée.
Ceci est illustré en Figure~\ref{fig:sc_r}, toujours en considérant l'algorithme SC. La tendance observée est bien que plus le rendement est faible, plus le taux d'erreur est faible.
Une seule exception apparaît, pour le rendement $R=1/8$, rendement le plus faible pourtant. Ceci est dû au fait que l'énergie par bit d'information $E_b$ prend en compte le fait que lorsqu'on diminue le rendement, on augmente l'énergie dépensée pour transmettre un bit utile. Deux tendances s'affrontent donc : augmentation de la redondance contre diminution du rapport signal à bruit.

\subsubsection{Impact des paramètres de l'algorithme SCL}

\begin{figure}[t]
  \centering
  \input{main/ch1_fig/curves/scl_L/tikz/source.tex}
  \caption{Performances de décodage des algorithmes SC, SCL et CASCL pour un code polaire (2048,1723). Un CRC de taille $c=16$ est utilisé pour l'algorithme CASCL.}
  \label{fig:scl_l}
\end{figure}
La Figure~\ref{fig:scl_l} présente les performances de l'algorithme SCL. Le code considéré est un code dont la taille du mot de code est 2048 et le nombre de bits d'information est 1723. La notation employée pour désigner un tel code est : (2048,1273). Comme précisé auparavant, l'algorithme SCL permet d'améliorer les performances de décodage des codes polaires par rapport à l'algorithme SC.
Plusieurs choses sont à observer. Tout d'abord, l'augmentation de la taille de la liste permet dans tous les cas une meilleure correction d'erreurs. Ensuite, l'utilisation de la concaténation du CRC par l'algorithme CASCL augmente très fortement les performances de décodage. Par exemple lorsque $L=32$, à un FER de 10\textsuperscript{-5}, il y a une différence de 1 dB entre les algorithmes SCL et CASCL.

Des tendances plus fines se dégagent. Les courbes de l'algorithme SCL montrent que les gains de performance résultant de l'augmentation du paramètre $L$ deviennent marginaux. En effet, les performances obtenues pour $L=8$ et $L=32$ sont très proches, alors que la complexité de l'algorithme augmente approximativement linéairement. En revanche, concernant l'algorithme CASCL, l'amélioration des performances due à l'augmentation du paramètre $L$ est continue et significative. Par exemple, pour $E_b/N_0=4dB$, le gain de FER entre $L=8$ et $L=32$ est de plus d'une décade.

Pour un code polaire concaténé avec un CRC donné, la taille du CRC a également un impact sur les performances de décodage. Là encore, deux phénomènes antagonistes s'affrontent. 
Tout d'abord, plus le CRC est long, moins il est probable que se produise une validation erronée d'une trame fausse, appelée fausse détection. Ceci apparaît pour des valeurs élevées de $E_b/N_0$ : les algorithmes pour lesquels $c$ est trop petit sont moins performants.
Cependant, sur la Figure~\ref{fig:scl_crc}, jusqu'à $E_b/N_0=4dB$, la version de l'algorithme la plus performante est celle pour laquelle $c=16$, et non pas $c=32$.
En effet, il est montré dans la Figure \ref{fig:crc} que le nombre de bits d'information par trame est $K$ mais que le nombre de bits à l'entrée de l'encodeur polaire est $K+c$. Autrement dit, bien que le rendement total du code concaténé soit $R=\dfrac{K}{N}$, le rendement du code polaire seul est $R=\dfrac{K+c}{N}$. Lorsque $c$ augmente, le rendement du code polaire seul augmente également, et ses performances de décodage diminuent. Par ailleurs, il est important de noter que pour une taille de CRC donnée, le polynôme choisi influe également sur les performances comme étudié dans \cite{zhang_crc_2017}.
\begin{figure}[t]
  \centering
  \subfloat[Algorithmes non adaptatif.]{
  \input{main/ch1_fig/curves/scl_crc/tikz/source.tex}
  \label{fig:scl_crc}
  }
  \subfloat[Algorithmes adaptatifs.]{
  \input{main/ch1_fig/curves/fast_ascl_crc/tikz/source.tex}
  \label{fig:ascl_crc}
  }
  \caption{Impact de la taille du CRC sur les performances de l'algorithme SCL pour un code (2048,1723).}
\end{figure}

L'impact de la taille du CRC sur les performances de décodage des algorithmes adaptatifs est encore plus important. Celles-ci sont représentées pour un code polaire (2048,1723) et différentes tailles de CRC. En utilisant un CRC de taille $c=8$ ou $c=16$ on observe des dégradations importantes des performances entre les algorithmes adaptatifs d'une part et l'algorithme SCL de base d'autre part. La différence entre PASCL et FASCL est en outre négligeable. Ce phénomène s'explique par le fait qu'à des FER très faibles, il devient davantage probable d'obtenir des faux positifs lors du test de CRC durant premier décodage SC dans le cadre des algorithmes adaptatifs. Toutefois, pour un CRC de taille $c=32$, il n'y a aucune différence de performances pour le code polaire étudié entre les algorithmes CASCL, PASCL et FASCL.

\begin{figure}[h]
  \centering
  \input{main/ch1_fig/curves/scf/tikz/source.tex}
  \caption{Performances de l'algorithme SCF en comparaison avec les algorithmes SC et SCL pour un code polaire (1024,512), $c=16$.}
  \label{fig:scf}
\end{figure}
\begin{figure}[h]
  \centering
  \input{main/ch1_fig/curves/bp_scan/tikz/source.tex}
  \caption{Performances des algorithmes itératifs à sortie souple en comparaison avec les algorithmes SC et SCL pour un code polaire (1024,512).}
  \label{fig:bp_scan}
\end{figure}
\subsubsection{Performances de décodages des algorithmes SCAN et SCF}
Les Figures \ref{fig:scf} et \ref{fig:bp_scan} présentent les performances de décodages de ces deux algorithmes pour un code polaire (1024,512). Les courbes de performances pour l'algorithme SCF sont issues de \cite{afisiadis_low-complexity_2014} tandis que les courbes de performances de l'algorithme BP sont issues de \cite{pamuk_fpga_2011}. Les performances du SCF s'améliorent lorsque $T$ augmente mais restent inférieures aux performances de l'algorithme SCL avec $L=4$. Le même ordre de grandeur est observé pour un algorithme SCAN à 4 itérations. Il est notable de remarquer que les performances du SCAN avec 4 itérations sont bien meilleures que celles de l'algorithme BP pour 50 itérations. Ceci s'explique par le fait qu'à l'inverse du décodage SCAN, le séquencement du décodage BP ne tire aucun bénéfice du phénomène de polarisation car chaque bit est estimé en parallèle.

\section{L'élagage de l'arbre de décodage}

% Figure sous-arbre

\subsection{L'élagage de l'algorithme de décodage SC}
Afin de réduire la complexité calculatoire de l'algorithme de décodage SC, il est possible d'élaguer l'arbre de décodage comme indiqué dans \cite{alamdar-yazdi_simplified_2011}. En effet, un sous-arbre dont toutes les feuilles sont de rendement 0 n'a pas besoin d'être parcouru. Comme toutes ses feuilles sont des bits gelés, alors toutes les sommes partielles du sous-arbre sont nulles. Le même élagage est possible pour des sous-arbres dont toutes les feuilles sont de rendement 1. Pour un tel sous-arbre, il suffit d'appliquer la fonction de seuillage \texttt{R1} sur chaque LLR de la racine du sous-arbre en question pour obtenir le même résultat que s'il avait été parcouru dans sa totalité.

Deux autres types de sous-arbres peuvent être identifiés \cite{sarkis_fast_2014}, et des fonctions spécialisées permettent d'obtenir les valeurs des sommes partielles directement depuis les valeurs des LLR de la racine de ces types de sous-arbres. Soit un sous-arbre dont le \noeud racine a une taille $M$. Si $M-1$ feuilles correspondent à des bits gelés et la dernière à un bit d'information, le sous-arbre correspond à un \noeud de répétition. Les valeurs des sommes partielles d'un tel \noeud sont toutes égales à $0$ ou toutes égales à $1$. Pour déterminer quelles valeurs prennent les sommes partielles, tous les LLR du \noeud considéré sont additionnés. Si le total est supérieur à 0, alors toutes les sommes partielles sont mises à $0$, et dans le cas contraire, toutes à $1$.

Le dernier type de sous-arbre correspond à un \noeud à test de parité unique (SPC). Plusieurs étapes sont nécessaires au traitement d'un tel \noeud. Ce traitement est noté \texttt{REP} dans la suite du document. Tout d'abord, la fonction élémentaire \texttt{R1} est appliquée sur chaque LLR, comme pour un \noeud de rendement 1. La parité de ce premier vecteur de sommes partielles est testée. Si celle-ci est égale à 0, le test de parité est satisfait, et les sommes partielles ne sont pas modifiées. Dans le cas contraire, les LLR sont triés selon leur valeur absolue respective. La somme partielle associée au LLR ayant la valeur absolue la plus faible est inversée. Le vecteur ainsi corrigé respecte cette fois la contrainte de parité. La Figure \ref{fig:sc_pruned} détaille un exemple d'élagage de l'arbre de décodage SC, avec deux \noeuds \texttt{R0} et \texttt{R1} chacun de taille 2 et un \noeud SPC de taille 4. L'intérêt de l'élagage apparaît ici clairement : le nombre de fonctions appliquées, matérialisées par des flèches, diminue. Qui plus est, le parcours de l'arbre nécessite des calculs, surtout pour des implémentations logicielles. La détermination du branchement correspond, selon le type de description logicielle, soit à un appel récursif de fonction, soit à des calculs d'adresse, puis à de indirections. De plus, diminuer la taille de l'arbre réduit la complexité calculatoire de contrôle.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{main/ch1_fig/sc_tree_pruned.pdf}
  \caption{\'Elagage de l'arbre de décodage SC.}
  \label{fig:sc_pruned}
\end{figure}

\subsection{L'élagage de l'algorithme de décodage SCL}
\label{subsec:pruning}

Le mécanisme d'élagage de l'arbre peut être décliné pour l'algorithme SCL, il est toutefois plus complexe à mettre en œuvre. Dans l'algorithme SCL classique, une duplication du décodeur est nécessaire à chaque fois qu'un bit d'information est estimé. Dans le cadre d'un arbre élagué, pour tous les types de sous-arbres (exceptés ceux de rendement 0), il faut générer plusieurs candidats pour le processus de duplication. Les mécanisme de traitement de chaque type de \noeud sont énumérés ci-dessous. Pour chaque type de \noeud, la méthode de génération des candidats est exposée, accompagnée du calcul de leurs métriques respectives.

Dans un \noeud de rendement 0, comme pour le SC, les sommes partielles de sortie d'un sous-arbre sont toutes égales à 0. Il n'y a pas de nouveaux candidats à générer. Par contre, les métriques doivent être mises à jour. Le traitement pour chaque LLR en entrée du sous-arbre est le même que pour le traitement d'une feuille de rendement 0. S'il est positif, aucune pénalité n'est appliquée à la métrique du chemin courant. S'il est négatif, une pénalité égale à la valeur absolue du LLR est ajoutée à la métrique. 

Pour le \noeud de répétition, puisqu'un seul bit d'information est présent dans le sous-arbre, seulement deux versions des sommes partielles en sortie sont possibles, soit toutes à 1, soit toutes à 0. Ces deux versions constituent les deux candidats issus de chaque chemin. A chacun de ces candidats doit être affectée une nouvelle métrique. 
Pour le candidat \og tout à 0 \fg, la valeur absolue de chaque LLR négatif du \noeud est ajoutée à la métrique du chemin. 
Pour le candidat \og tout à 1 \fg, la valeur absolue de chaque LLR positif du \noeud est ajoutée à la métrique du chemin. 

Le traitement du \noeud de rendement 1 est plus intensif en calcul. En effet, si le \noeud est de taille $T$, alors le nombre de candidats possibles est $2^T$. Pour chaque candidat, il faut calculer la nouvelle métrique. Ce calcul est le traitement \texttt{R1} appliqué à chaque LLR en entrée du sous-arbre, c'est-à-dire un seuillage. Comme pour tout \noeud terminal, il faut ensuite trier les $2^T$ candidats afin d'en sélectionner les $L$ ayant les métriques les plus faibles. Malheureusement, ces traitements deviennent trop intensifs pour une implémentation réaliste. Une diminution drastique de la complexité de ce traitement peut être obtenue par l'utilisation de l'algorithme \og Chase \fg \cite{chase_class_1972} sur le sous-arbre. Tout d'abord, le premier candidat est obtenu en appliquant un seuillage sur les $T$ LLR d'entrée à la manière du traitement du \noeud de rendement 1 dans l'algorithme SC. Ensuite les $T$ LLR d'entrée sont tout d'abord triés selon leur valeur absolue respective. Les deux LLR parmi les $T$  dont les valeurs sont les plus faibles sont identifiés, notés LLR\textsubscript{1} et LLR\textsubscript{2}. Les deuxième et troisième candidats sont générés en inversant la somme partielle associée au LLR\textsubscript{1} et celle associée au LLR\textsubscript{2}. Le quatrième candidat est généré en inversant les deux sommes partielles à la fois. La métrique de chaque candidat est calculée en ajoutant à la métrique du chemin considéré la valeur absolue des LLR dont la somme partielle a été inversée. Cet heuristique permet d'atteindre des performances très proches de celles d'une version non simplifiée. Il est possible que des dégradations de performance apparaissent, puisque le nombre de candidats envisagés pour un sous-arbre est réduit par rapport à la version non élaguée de l'algorithme SCL. Aucune dégradation notable n'a néanmoins été observée dans les faits expérimentaux.

 

Enfin, le traitement des \noeuds \texttt{SPC} est similaire au traitement du \noeud de rendement 1 à la différence qu'au lieu de deux bits potentiellement inversés, ce sont quatre bits qui sont à considérer. De plus, seuls les candidats respectant la contrainte de parité sont générés. A la différence des \noeuds de rendement 1, la performance de correction de l'algorithme élagué peut être dégradée. En effet, dans \cite{sarkis_fast_2016}, des dégradations importantes ont été observées pour des \noeuds \texttt{SPC} de taille supérieure à 4. 


% détailler calculs évitables (g0, f0, grep)
% discussion chase spc - r1 ? Hashemi-Sarkis + Simulations

\subsection{L'élagage de l'algorithme de décodage SCAN}
Tout comme les algorithmes SC et SCL, des élagages peuvent être appliqués à l'algorithme SCAN \cite{lin_reduced_2015}. \`A la différence des deux précédents, les \noeuds \texttt{REP} et \texttt{SPC} ne sont pas élagués. De plus, le concept de sous-arbre dans l'algorithme SCAN évolue. Tandis que dans l'algorithme SC, un sous-arbre prenait comme entrée des LLR et produisait en sortie des sommes partielles, dans l'algorithme SCAN, les sorties des sous-arbres sont également des LLR. Dans le cas d'un rendement 0, les sorties du \noeud sont toutes à $+\infty$, puisqu'il est certain que les bits du mots de code sont des $0$. Dans le cas d'un rendement 1, les sorties du \noeud sont tous à $0$ puisqu'en l'absence de redondance, aucune information ne peut être donnée à partir de ce sous-arbre.

% \section{Codes polaires de taille variables}
% https://arxiv.org/pdf/1701.06458.pdf 4-5-6-7-8-9


\section{Synthèse des différents algorithmes de décodage}
 Dans ce chapitre ont été présentés les codes polaires, avec une emphase particulière sur les différents algorithmes de décodage. Le Tableau \ref{tab:algo} récapitule ces différents algorithmes, en donnant une indication approximative sur leurs performances en termes de correction d'erreur, débit et latence maximum.
  \begin{table}[htp]
    \centering
    \caption{Tendances des différents algorithmes concernant leurs pouvoirs de correction, débits et latences.}
    \label{tab:algo}
    {\small\resizebox{\linewidth}{!}{
     \begin{tabular}{r|c|c|c|c}
      \textbf{Algorithme}  & \textbf{Performances}  & \multirow{1}{*}{\textbf{Débit}} & \textbf{Latence max.}             & \textbf{Sorties} \\
      \textbf{de décodage} & \textbf{BER \& FER}   & ($\bm{\mathcal{T}}$)                 & ($\bm{\mathcal{L}_{worst}}$)  & \textbf{souples} \\
      \hline
      SC        & faibles     & très élevé    & très faible    & non \\
      SCF       & moyennes    & élevé         & faible         & non \\
      SCL       & moyennes    & faible        & élevée         & non \\
      SCS       & élevées     & faible        & élevée         & non \\
      CASCL     & élevées     & faible        & élevée         & non \\
      PASCL     & élevées     & élevé         & élevée         & non \\
      FASCL     & élevées     & élevé         & très élevée    & non \\
      \hline
      BP        & faibles     & élevé         & faible         & oui \\
      SCAN      & faibles     & élevé         & faible         & oui \\
    \end{tabular}
    }}
  \end{table}



Les meilleures performances de décodage sont atteintes par les algorithmes à liste utilisant un CRC concaténés (CASCL, PASCL, FASCL) ainsi que par l'algorithme SCS. Toutefois, la complexité de ces algorithmes a pour conséquence une latence élevée de leurs implémentations. L'algorithme SC est au contraire très peu complexe mais présente des performances de décodage plus faibles. Les algorithmes BP et SCAN sont des algorithmes à sortie souple. Leurs performances brutes sont faibles, mais ils peuvent être utilisés dans des schémas de codages concaténés avec échange d'information souple. Le chapitre suivant porte sur une implémentation logicielle des algorithmes à liste.